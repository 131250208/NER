{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NER'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b687780f9998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myucheng_ner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m from yucheng_ner.tplinker_ner.tplinker_ner import (HandshakingTaggingScheme,\n\u001b[1;32m     22\u001b[0m                                               \u001b[0mDataMaker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/bigdata/wycheng_backup/research/ner_tp/yucheng_ner/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myucheng_ner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtplinker_ner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtplinker_ner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myucheng_ner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_common\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/bigdata/wycheng_backup/research/ner_tp/yucheng_ner/tplinker_ner/tplinker_ner.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHandshakingKernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'NER'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "import yaml\n",
    "import os\n",
    "config = yaml.load(open(\"extractor_config.yaml\", \"r\"), Loader = yaml.FullLoader)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"device_num\"])\n",
    "import json\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from yucheng_ner.ner_common.utils import Preprocessor\n",
    "from yucheng_ner.tplinker_ner.tplinker_ner import (HandshakingTaggingScheme,\n",
    "                                              DataMaker, \n",
    "                                              TPLinkerNER)\n",
    "from collections import OrderedDict\n",
    "from Doraemon import whois, domain2ip\n",
    "from IPython.core.debugger import set_trace\n",
    "from bs4 import BeautifulSoup\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>data path>>>>>>>>>>>>>>\n",
    "data_home = config[\"data_home\"]\n",
    "in_file_name = config[\"in_file_name\"]\n",
    "out_file_name = config[\"out_file_name\"]\n",
    "\n",
    "# >>>>>>>>predication config>>>>>>\n",
    "max_seq_len = config[\"max_seq_len\"]\n",
    "sliding_len = config[\"sliding_len\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "\n",
    "# >>>>>>>>>>model config>>>>>>>>>>>>\n",
    "## bert encoder\n",
    "bert_config = config[\"bert_config\"] if config[\"use_bert\"] else None\n",
    "bert_config[\"path\"] = data_home + \"/bert-base-cased\"\n",
    "use_bert = config[\"use_bert\"]\n",
    "\n",
    "## handshaking_kernel\n",
    "handshaking_kernel_config = config[\"handshaking_kernel_config\"]\n",
    "visual_field = handshaking_kernel_config[\"visual_field\"]\n",
    "\n",
    "## encoding fc\n",
    "enc_hidden_size = config[\"enc_hidden_size\"]\n",
    "activate_enc_fc = config[\"activate_enc_fc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names_abbr = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\",\n",
    "                    \"KY\",\n",
    "                    \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\",\n",
    "                    \"ND\",\n",
    "                    \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\",\n",
    "                    ]\n",
    "\n",
    "state_names = [\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\",\n",
    "               \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\",\n",
    "               \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\",\n",
    "               \"Missouri\",\n",
    "               \"Montana\", \"Nebraska\", \"Nevada\", \"New hampshire\", \"New Jersey\", \"New Mexico\", \"New York\",\n",
    "               \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\",\n",
    "               \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\",\n",
    "               \"West Virginia\", \"Wisconsin\", \"Wyoming\",\n",
    "               ]\n",
    "state_abbr_2_full_name = dict(zip(state_names_abbr, state_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_html(html):\n",
    "    text = BeautifulSoup(html).get_text(separator=\" \")\n",
    "    return re.sub(\"\\s+\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr2fullname(text):\n",
    "    # replace state abbrivation with full name \n",
    "    sub_dict = {}\n",
    "    for m in re.finditer(\"([A-Z]{2}) (\\d{5})\", text):\n",
    "        state_abbr, zipcode = m.group(1), m.group(2)\n",
    "        if state_abbr not in state_abbr_2_full_name:\n",
    "            continue\n",
    "        sub_dict[\"{} {}\".format(state_abbr, zipcode)] = \"{} {}\".format(state_abbr_2_full_name[state_abbr], zipcode)\n",
    "\n",
    "    for ori_str, sub_str in sub_dict.items():\n",
    "        text = re.sub(re.escape(ori_str), sub_str, text)\n",
    "    return text \n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"©|(&copy;)|copyright|Copyright|（c）|\\(c\\)\", \" copyright \", text)# 替换所有的copyright为统一字符串\n",
    "    # 用空格将单词、数字、其他字符切开，do not split decimal point \n",
    "    text = re.sub(\"([^A-Za-z0-9\\.])\", r\" \\1 \", text)\n",
    "    text = re.sub(\"(\\D)\\.(.)\", r\"\\1 . \\2\", text)\n",
    "    text = re.sub(\"(.)\\.(\\D)\", r\"\\1 . \\2\", text)\n",
    "    # 切开错误连接的单词\n",
    "    text = re.sub(\"([A-Z]{1}[a-z]+)([A-Z]{1}[a-z]+)\", r\"\\1 \\2\", text)\n",
    "    # remove redundant blanks\n",
    "    text = re.sub(\"\\s+\", \" \", text).strip() \n",
    "    \n",
    "    text = abbr2fullname(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert tokenizer\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_config[\"path\"], add_special_tokens = False, do_lower_case = False)\n",
    "preprocessor = Preprocessor(bert_tokenizer, True)\n",
    "# split function\n",
    "def split(data, max_seq_len, sliding_len, data_name = \"train\"):\n",
    "    '''\n",
    "    split into short texts\n",
    "    '''\n",
    "    max_tok_num = 0\n",
    "    for sample in tqdm(data, \"calculating the max token number of {}\".format(data_name)):\n",
    "        text = sample[\"text\"]\n",
    "        tokens = preprocessor.tokenize(text)\n",
    "        max_tok_num = max(max_tok_num, len(tokens))\n",
    "    print(\"max token number of {}: {}\".format(data_name, max_tok_num))\n",
    "    \n",
    "    if max_tok_num > max_seq_len:\n",
    "        print(\"max token number of {} is greater than the setting, need to split!\".format(data_name, data_name, max_seq_len))\n",
    "        short_data = preprocessor.split_into_short_samples(data, \n",
    "                                          max_seq_len, \n",
    "                                          sliding_len = sliding_len, \n",
    "                                          data_type = \"test\")\n",
    "    else:\n",
    "        short_data = data\n",
    "        max_seq_len = max_tok_num\n",
    "        print(\"max token number of {} is less than the setting, no need to split!\".format(data_name, data_name, max_tok_num))\n",
    "    return short_data, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "page_data_path = os.path.join(data_home, in_file_name)\n",
    "pages = json.load(open(page_data_path, \"r\", encoding = \"utf-8\"))\n",
    "# for page in pages:\n",
    "#     page[\"text\"] = preprocess(page[\"text\"])\n",
    "\n",
    "# domain to ip\n",
    "url_list = [p[\"url\"] for p in pages if \"ip\" not in p]\n",
    "domain_name2ip = {}\n",
    "domain2ip.gethostbyname_fast(url_list, domain_name2ip)\n",
    "print(\"ip_num: {}\".format(len(domain_name2ip)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "for idx, page in tqdm(enumerate(pages), desc = \"normalizing\"):\n",
    "    page[\"id\"] = idx\n",
    "    assert \"url\" in page and (\"html\" in page or \"text\" in page)\n",
    "\n",
    "    if \"ip\" not in page:\n",
    "        dn = domain2ip.urlfillter(page[\"url\"])\n",
    "        page[\"ip\"] = domain_name2ip[dn]\n",
    "\n",
    "    if \"text\" not in page:\n",
    "        page[\"text\"] = preprocess(drop_html(page[\"html\"]))\n",
    "    else:\n",
    "        page[\"text\"] = preprocess(page[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "ori_pages = copy.deepcopy(pages)\n",
    "short_pages_, max_seq_len = split(ori_pages, max_seq_len, sliding_len, \"pages.json\")\n",
    "print(\"final max_seq_len is {}\".format(max_seq_len))\n",
    "\n",
    "# filter\n",
    "key_words = [\"all right reserved\", \"&copy;\", \"©\", \"copyright\"]\n",
    "short_pages = []\n",
    "for page_ in tqdm(short_pages_, desc = \"filtering pages wo keywords\"):\n",
    "    page = copy.deepcopy(page_)\n",
    "    if re.search(\"|\".join(key_words), page[\"text\"], flags = re.I):\n",
    "        short_pages.append(page)\n",
    "    elif re.search(\"({})\".format(\"|\".join(state_names)) + \"\\s+\\d{5}\", page[\"text\"]):\n",
    "        short_pages.append(page)\n",
    "\n",
    "print(\"Valid page number: {}\".format(len(short_pages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder(Tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"city\", \"detail\", \"organization\", \"state\", \"zipcode\"]\n",
    "handshaking_tagger = HandshakingTaggingScheme(tags, max_seq_len, visual_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max word num, max subword num, max char num\n",
    "def cal_max_tok_num(data, tokenizer):\n",
    "    max_tok_num = 0\n",
    "    for example in data:\n",
    "        text = example[\"text\"]\n",
    "        max_tok_num = max(max_tok_num, len(tokenizer.tokenize(text)))\n",
    "    return max_tok_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_subword_num = cal_max_tok_num(short_pages, bert_tokenizer)\n",
    "print(\"max_subword_num: {}\".format(max_subword_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_maker = DataMaker(handshaking_tagger, None, bert_tokenizer, None, None, max_subword_num, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_extractor = TPLinkerNER(None,\n",
    "                            None,\n",
    "                            None,\n",
    "                            handshaking_kernel_config,\n",
    "                            enc_hidden_size,\n",
    "                            activate_enc_fc,\n",
    "                            len(tags),\n",
    "                            bert_config,\n",
    "                            )\n",
    "ent_extractor = ent_extractor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(ent_list):\n",
    "    ent_memory_set = set()\n",
    "    filtered_ent_list = []\n",
    "    for ent in ent_list:\n",
    "        ent_memory = \"{}\\u2E80{}\\u2E80{}\".format(ent[\"tok_span\"][0], ent[\"tok_span\"][1], ent[\"type\"])\n",
    "        if ent_memory not in ent_memory_set:\n",
    "            filtered_ent_list.append(ent)\n",
    "            ent_memory_set.add(ent_memory)\n",
    "    return filtered_ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_dataloader, ori_test_data):\n",
    "    '''\n",
    "    test_data: if split, it would be samples with subtext\n",
    "    ori_test_data: the original data has not been split, used to get original text here\n",
    "    '''\n",
    "    pred_sample_list = []\n",
    "    for batch_test_data in tqdm(test_dataloader, desc = \"Predicting\"):\n",
    "        sample_list = batch_test_data[\"sample_list\"]\n",
    "        tok2char_span_list = batch_test_data[\"tok2char_span_list\"]\n",
    "        del batch_test_data[\"sample_list\"]\n",
    "        del batch_test_data[\"tok2char_span_list\"]\n",
    "\n",
    "        for k, v in batch_test_data.items():\n",
    "            if k not in {\"padded_sents\"}:\n",
    "                batch_test_data[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_pred_shaking_outputs = ent_extractor(**batch_test_data)\n",
    "        batch_pred_shaking_tag = (batch_pred_shaking_outputs > 0.).long()\n",
    "\n",
    "        for ind in range(len(sample_list)):\n",
    "            sample = sample_list[ind]\n",
    "            text = sample[\"text\"]\n",
    "            text_id = sample[\"id\"]\n",
    "            tok2char_span = tok2char_span_list[ind]\n",
    "            pred_shaking_tag = batch_pred_shaking_tag[ind]\n",
    "            tok_offset, char_offset = 0, 0\n",
    "            tok_offset, char_offset = (sample[\"tok_offset\"], sample[\"char_offset\"]) if \"char_offset\" in sample else (0, 0)\n",
    "            ent_list = handshaking_tagger.decode_ent(text, \n",
    "                                                     pred_shaking_tag, \n",
    "                                                     tok2char_span, \n",
    "                                                     tok_offset = tok_offset, \n",
    "                                                     char_offset = char_offset)\n",
    "            pred_sample_list.append({\n",
    "                \"text\": text,\n",
    "                \"id\": text_id,\n",
    "                \"entity_list\": ent_list,\n",
    "            })\n",
    "            \n",
    "    # merge\n",
    "    text_id2ent_list = {}\n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        if text_id not in text_id2ent_list:\n",
    "            text_id2ent_list[text_id] = sample[\"entity_list\"]\n",
    "        else:\n",
    "            text_id2ent_list[text_id].extend(sample[\"entity_list\"])\n",
    "\n",
    "    text_id2text = {sample[\"id\"]:sample[\"text\"] for sample in ori_test_data}\n",
    "    page_id2ip= {sample[\"id\"]:sample[\"ip\"] for sample in ori_test_data}\n",
    "    \n",
    "    merged_pred_sample_list = []\n",
    "    for text_id, ent_list in text_id2ent_list.items():\n",
    "        merged_pred_sample_list.append({\n",
    "            \"id\": text_id,\n",
    "            \"ip\": page_id2ip[text_id],\n",
    "            \"text\": text_id2text[text_id],\n",
    "            \"entity_list\": filter_duplicates(ent_list),\n",
    "        })\n",
    "        \n",
    "    return merged_pred_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "indexed_test_data = data_maker.get_indexed_data(short_pages, data_type = \"test\")\n",
    "test_dataloader = DataLoader(MyDataset(indexed_test_data), \n",
    "                          batch_size = batch_size, \n",
    "                          shuffle = False, \n",
    "                          num_workers = 6,\n",
    "                          drop_last = False,\n",
    "                          collate_fn = lambda data_batch: data_maker.generate_batch(data_batch, data_type = \"test\"),\n",
    "                         )\n",
    "# load model\n",
    "model_path = os.path.join(data_home, config[\"model_dict\"])\n",
    "model_state_dict = torch.load(model_path)\n",
    "\n",
    "# if used paralell train, need to rm prefix \"module.\"\n",
    "new_model_state_dict = OrderedDict()\n",
    "for key, v in model_state_dict.items():\n",
    "    key = re.sub(\"module\\.\", \"\", key)\n",
    "    new_model_state_dict[key] = v\n",
    "ent_extractor.load_state_dict(new_model_state_dict)\n",
    "ent_extractor.eval()\n",
    "\n",
    "# predict\n",
    "pred_sample_list = predict(test_dataloader, ori_pages)\n",
    "\n",
    "page_num_w_ents = len([s for s in pred_sample_list if len(s[\"entity_list\"]) > 0])\n",
    "print(\"pages_with_entities/total_pages: {}/{}\".format(page_num_w_ents, len(ori_pages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dict-based Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERbyDict(object):\n",
    "    def __init__(self, prefix_2_orgs_dict):\n",
    "        self.dict = prefix_2_orgs_dict\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = re.sub(\"([A-Za-z0-9\\-\\']+)\", r\" \\1 \", text)\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        return text.split(\" \")\n",
    "    \n",
    "    def ner(self, target_str):\n",
    "        tokens = self.tokenize(target_str)\n",
    "        org_name2score = {}\n",
    "        copyright_inds = []\n",
    "        \n",
    "        for idx, t in enumerate(tokens):\n",
    "            if re.search(\"&copy;|©|copyright\", t, flags = re.I):\n",
    "                copyright_inds.append([idx, idx + 1])\n",
    "            if \" \".join(tokens[idx: idx + 3]).lower() == \"all right reserved\": \n",
    "                copyright_inds.append([idx, idx + 3])\n",
    "                \n",
    "        for ind, s in enumerate(tokens):\n",
    "            if s in self.dict: # if a token exists in dict(prefix)\n",
    "                org_names_2_len = self.dict[s]\n",
    "                \n",
    "                for length in sorted(set(org_names_2_len.values()), reverse=True): # sorted org name length in descending order\n",
    "                    if ind + length > len(tokens) or length == 1: # skip excessive length; org name with len == 1 in the dict is unreliable\n",
    "                        continue\n",
    "                    start, end = ind, ind + length\n",
    "                    substr = \" \".join(tokens[start: end])\n",
    "                    if substr in org_names_2_len: # hit\n",
    "                        score = len(tokens[start: end]) + len(substr) * 0.1\n",
    "                        if copyright_inds is not None:\n",
    "                            for cpr_pos in copyright_inds:\n",
    "                                dis = max(cpr_pos[0] - end, start - cpr_pos[1])\n",
    "                                score += 100 / (dis + 1)    \n",
    "                        org_name2score[substr] = max(org_name2score.get(substr, 0), score)\n",
    "\n",
    "        return sorted(org_name2score.items(), key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading org name dicts...\")\n",
    "with open(os.path.join(data_home, \"org_name_dict_uni.json\"), \"r\", encoding=\"utf-8\") as dict_uni, \\\n",
    "    open(os.path.join(data_home, \"org_name_dict_sb.json\"), \"r\", encoding=\"utf-8\") as dict_sb:\n",
    "    ner_dict = {**json.load(dict_uni), **json.load(dict_sb)}\n",
    "print(\"Org name dictionary is loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_dict_extractor = NERbyDict(ner_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def owner_org_extract(text):\n",
    "    orgs = org_dict_extractor.ner(text)\n",
    "    owner = orgs[0][0] if len(orgs) > 0 else None\n",
    "    return owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2org_name_by_dict = {}\n",
    "for sample in tqdm(pred_sample_list, \"extracting org name by dict\"):\n",
    "    org = owner_org_extract(sample[\"text\"])\n",
    "    if org is not None:\n",
    "        id2org_name_by_dict[sample[\"id\"]] = org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip2org_webpage = {}\n",
    "for sample in pred_sample_list:\n",
    "    text = sample[\"text\"]\n",
    "    \n",
    "    org2span = {}\n",
    "    for ent in sample[\"entity_list\"]:\n",
    "        if ent[\"type\"] == \"organization\":\n",
    "            org2span[ent[\"text\"]] = ent[\"char_span\"]\n",
    "\n",
    "    # choose an org\n",
    "    cpr_spans = [m.span() for m in re.finditer(\"&copy;|©|copyright|all right reserved\", text, flags = re.I)]\n",
    "    org2score = {org:0 for org, span in org2span.items()}\n",
    "    if len(cpr_spans) > 0:\n",
    "        for org, span in org2span.items():\n",
    "            score = 0\n",
    "            for cpr_pos in cpr_spans:\n",
    "                dis = max(cpr_pos[0] - span[1], span[0] - cpr_pos[1])\n",
    "                score += 100 / (dis + 1)  \n",
    "            org2score[org] += score\n",
    "    orgs = list(sorted(org2score.items(), key = lambda x: x[1], reverse = True))\n",
    "    if len(orgs) > 0:\n",
    "        org = orgs[0][0]\n",
    "    else:\n",
    "        org = id2org_name_by_dict[sample[\"id\"]] if sample[\"id\"] in id2org_name_by_dict else None\n",
    "        \n",
    "    ip2org_webpage[sample[\"ip\"]] = org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_list = list(domain_name2ip.values())\n",
    "ip_info_list = whois.extract_org_names_friendly(ip_list)\n",
    "\n",
    "ip2org_whois = {}\n",
    "for ip in ip_info_list:\n",
    "    if ip is not None:\n",
    "        ip2org_whois[ip[\"ip\"]] = ip[\"org_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip2loc_clues = {}\n",
    "for sample in pred_sample_list:\n",
    "    ip = sample[\"ip\"]\n",
    "    ip2loc_clues[ip] = []\n",
    "    for ent in sample[\"entity_list\"]:\n",
    "        ip2loc_clues[ip].append({\n",
    "            \"type\": ent[\"type\"],\n",
    "            \"text\": ent[\"text\"],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(ent_list):\n",
    "    new_ent_list = []\n",
    "    memory = set()\n",
    "    for ent in ent_list:\n",
    "        if str(ent) not in memory:\n",
    "            memory.add(str(ent))\n",
    "            new_ent_list.append(ent)\n",
    "    return new_ent_list\n",
    "\n",
    "ip2loc_clues_final = {}\n",
    "for ip, org_page in ip2org_webpage.items():\n",
    "    if ip in ip2org_whois:\n",
    "        org_whois = ip2org_whois[ip]\n",
    "        if org_page in  org_whois:\n",
    "            ip2loc_clues_final[ip] = {\n",
    "                \"org\": org_whois,\n",
    "                \"clues_on_webpage\": unique(ip2loc_clues[ip]),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(data_home, out_file_name)\n",
    "with open(save_path, \"w\", encoding = \"utf-8\") as file_out:\n",
    "    json.dump(ip2loc_clues_final, file_out, ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
