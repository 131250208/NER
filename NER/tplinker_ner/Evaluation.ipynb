{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "import yaml\n",
    "import os\n",
    "config = yaml.load(open(\"eval_config.yaml\", \"r\"), Loader = yaml.FullLoader)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"device_num\"])\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "from pprint import pprint\n",
    "import unicodedata\n",
    "from transformers import AutoModel, BasicTokenizer, BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import time\n",
    "from ner_common.utils import Preprocessor, WordTokenizer\n",
    "from tplinker_ner import (HandshakingTaggingScheme,\n",
    "                          DataMaker, \n",
    "                          TPLinkerNER,\n",
    "                          Metrics)\n",
    "import wandb\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count(), \"GPUs are available\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproductivity\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# >>>>>>>>predication config>>>>>>\n",
    "max_seq_len = config[\"max_seq_len\"]\n",
    "sliding_len = config[\"sliding_len\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "\n",
    "# >>>>>>>>>>model config>>>>>>>>>>>>\n",
    "## char encoder\n",
    "max_char_num_in_tok = config[\"max_char_num_in_tok\"]\n",
    "char_encoder_config = None\n",
    "if config[\"use_char_encoder\"]:\n",
    "    char_encoder_config = config[\"char_encoder_config\"]\n",
    "    char_encoder_config[\"max_char_num_in_tok\"] = max_char_num_in_tok\n",
    "\n",
    "## bert encoder\n",
    "bert_config = config[\"bert_config\"] if config[\"use_bert\"] else None\n",
    "use_bert = config[\"use_bert\"]\n",
    "\n",
    "## word encoder\n",
    "word_encoder_config = config[\"word_encoder_config\"] if config[\"use_word_encoder\"] else None\n",
    "\n",
    "## flair config\n",
    "flair_config = {\n",
    "    \"embedding_ids\": config[\"flair_embedding_ids\"],\n",
    "} if config[\"use_flair\"] else None\n",
    "\n",
    "## handshaking_kernel\n",
    "handshaking_kernel_config = config[\"handshaking_kernel_config\"]\n",
    "\n",
    "## encoding fc\n",
    "enc_hidden_size = config[\"enc_hidden_size\"]\n",
    "activate_enc_fc = config[\"activate_enc_fc\"]\n",
    "\n",
    "# >>>>>>>>>data path>>>>>>>>>>>>>>\n",
    "data_home = config[\"data_home\"]\n",
    "exp_name = config[\"exp_name\"]\n",
    "meta_path = os.path.join(data_home, exp_name, config[\"meta\"])\n",
    "save_res_dir = os.path.join(config[\"save_res_dir\"], exp_name)\n",
    "test_data_path = os.path.join(data_home, exp_name, config[\"test_data\"])\n",
    "word2idx_path = os.path.join(data_home, exp_name, config[\"word2idx\"])\n",
    "char2idx_path = os.path.join(data_home, exp_name, config[\"char2idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path_dict = {}\n",
    "for path in glob.glob(test_data_path):\n",
    "    file_name = re.search(\"(.*?)\\.json\", path.split(\"/\")[-1]).group(1)\n",
    "    test_data_path_dict[file_name] = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict = {}\n",
    "for file_name, path in test_data_path_dict.items():\n",
    "    test_data_dict[file_name] = json.load(open(path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tokenizers\n",
    "if use_bert:\n",
    "    bert_tokenizer = BertTokenizerFast.from_pretrained(bert_config[\"path\"], add_special_tokens = False, do_lower_case = False)\n",
    "word2idx = json.load(open(word2idx_path, \"r\", encoding = \"utf-8\"))\n",
    "word_tokenizer = WordTokenizer(word2idx)\n",
    "\n",
    "# preprocessor\n",
    "tokenizer4preprocess = bert_tokenizer if use_bert else word_tokenizer\n",
    "preprocessor = Preprocessor(tokenizer4preprocess, use_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, max_seq_len, sliding_len, data_name = \"train\"):\n",
    "    '''\n",
    "    split into short texts\n",
    "    '''\n",
    "    max_tok_num = 0\n",
    "    for sample in tqdm(data, \"calculating the max token number of {}\".format(data_name)):\n",
    "        text = sample[\"text\"]\n",
    "        tokens = preprocessor.tokenize(text)\n",
    "        max_tok_num = max(max_tok_num, len(tokens))\n",
    "    print(\"max token number of {}: {}\".format(data_name, max_tok_num))\n",
    "    \n",
    "    if max_tok_num > max_seq_len:\n",
    "        print(\"max token number of {} is greater than the setting, need to split!\".format(data_name, data_name, max_seq_len))\n",
    "        short_data = preprocessor.split_into_short_samples(data, \n",
    "                                          max_seq_len, \n",
    "                                          sliding_len = sliding_len, \n",
    "                                          data_type = \"test\")\n",
    "    else:\n",
    "        short_data = data\n",
    "        max_seq_len = max_tok_num\n",
    "        print(\"max token number of {} is less than the setting, no need to split!\".format(data_name, data_name, max_tok_num))\n",
    "    return short_data, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = []\n",
    "# for data in list(test_data_dict.values()):\n",
    "#     all_data.extend(data)\n",
    "    \n",
    "# max_tok_num = 0\n",
    "# for sample in tqdm(all_data, desc = \"Calculate the max token number\"):\n",
    "#     tokens = tokenize(sample[\"text\"])\n",
    "#     max_tok_num = max(len(tokens), max_tok_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_test_data = False\n",
    "# if max_tok_num > config[\"max_test_seq_len\"]:\n",
    "#     split_test_data = True\n",
    "#     print(\"max_tok_num: {}, lagger than max_test_seq_len: {}, test data will be split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "# else:\n",
    "#     print(\"max_tok_num: {}, less than or equal to max_test_seq_len: {}, no need to split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "# max_seq_len = min(max_tok_num, config[\"max_test_seq_len\"]) \n",
    "\n",
    "# if config[\"force_split\"]:\n",
    "#     split_test_data = True\n",
    "#     print(\"force to split the test dataset!\")    \n",
    "\n",
    "ori_test_data_dict = copy.deepcopy(test_data_dict)\n",
    "test_data_dict = {}\n",
    "max_seq_len_all_data = []\n",
    "for file_name, data in ori_test_data_dict.items():\n",
    "    split_data, max_seq_len_this_data = split(data, max_seq_len, sliding_len, file_name)\n",
    "    max_seq_len_all_data.append(max_seq_len_this_data)\n",
    "    test_data_dict[file_name] = split_data\n",
    "max_seq_len = max(max_seq_len_all_data)\n",
    "print(\"final max_seq_len is {}\".format(max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, short_data in test_data_dict.items():\n",
    "    print(\"example number of {}: {}\".format(filename, len(short_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder(Tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = json.load(open(meta_path, \"r\", encoding = \"utf-8\"))\n",
    "tags = meta[\"tags\"]\n",
    "if meta[\"visual_field_rec\"] > handshaking_kernel_config[\"visual_field\"]:\n",
    "    handshaking_kernel_config[\"visual_field\"] = meta[\"visual_field_rec\"]\n",
    "    print(\"Recommended visual_field is greater than current visual_field, reset to rec val: {}\".format(handshaking_kernel_config[\"visual_field\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handshaking_tagger = HandshakingTaggingScheme(tags, max_seq_len, handshaking_kernel_config[\"visual_field\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = json.load(open(char2idx_path, \"r\", encoding = \"utf-8\"))\n",
    "def text2char_indices(text, max_seq_len = -1):\n",
    "    char_ids = []\n",
    "    chars = list(text)\n",
    "    for c in chars:\n",
    "        if c not in char2idx:\n",
    "            char_ids.append(char2idx['<UNK>'])\n",
    "        else:\n",
    "            char_ids.append(char2idx[c])\n",
    "    if len(char_ids) < max_seq_len:\n",
    "        char_ids.extend([char2idx['<PAD>']] * (max_seq_len - len(char_ids)))\n",
    "    if max_seq_len != -1:\n",
    "        char_ids = torch.tensor(char_ids[:max_seq_len]).long()\n",
    "    return char_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max word num, max subword num, max char num\n",
    "def cal_max_tok_num(data, tokenizer):\n",
    "    max_tok_num = 0\n",
    "    for example in data:\n",
    "        text = example[\"text\"]\n",
    "        max_tok_num = max(max_tok_num, len(tokenizer.tokenize(text)))\n",
    "    return max_tok_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for data in list(test_data_dict.values()):\n",
    "    all_data.extend(data)\n",
    "\n",
    "max_word_num = cal_max_tok_num(all_data, word_tokenizer)\n",
    "print(\"max_word_num: {}\".format(max_word_num))\n",
    "if use_bert:\n",
    "    max_subword_num = cal_max_tok_num(all_data, bert_tokenizer)\n",
    "    print(\"max_subword_num: {}\".format(max_subword_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_tokenizer = bert_tokenizer if use_bert else None\n",
    "data_maker = DataMaker(handshaking_tagger, word_tokenizer, subword_tokenizer, text2char_indices, \n",
    "                       max_word_num, max_subword_num, max_char_num_in_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if char_encoder_config is not None:\n",
    "    char_encoder_config[\"char_size\"] = len(char2idx)\n",
    "if word_encoder_config is not None:\n",
    "    word_encoder_config[\"word2idx\"] = word2idx\n",
    "ent_extractor = TPLinkerNER(char_encoder_config,\n",
    "                            word_encoder_config,\n",
    "                            flair_config,\n",
    "                            handshaking_kernel_config,\n",
    "                            enc_hidden_size,\n",
    "                            activate_enc_fc,\n",
    "                            len(tags),\n",
    "                            bert_config,\n",
    "                            )\n",
    "ent_extractor = ent_extractor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics(handshaking_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model state paths\n",
    "model_state_dir = config[\"model_state_dict_dir\"]\n",
    "target_run_ids = set(config[\"run_ids\"])\n",
    "run_id2model_state_paths = {}\n",
    "for root, dirs, files in os.walk(model_state_dir):\n",
    "    for file_name in files:\n",
    "        run_id = root.split(\"-\")[-1]\n",
    "        if re.match(\".*model_state.*\\.pt\", file_name) and run_id in target_run_ids:\n",
    "            if run_id not in run_id2model_state_paths:\n",
    "                run_id2model_state_paths[run_id] = []\n",
    "            model_state_path = os.path.join(root, file_name)\n",
    "            run_id2model_state_paths[run_id].append(model_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_k_paths(path_list, k):\n",
    "    path_list = sorted(path_list, key = lambda x: int(re.search(\"(\\d+)\", x.split(\"/\")[-1]).group(1)))\n",
    "#     pprint(path_list)\n",
    "    return path_list[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only last k models\n",
    "k = config[\"last_k_model\"]\n",
    "for run_id, path_list in run_id2model_state_paths.items():\n",
    "    run_id2model_state_paths[run_id] = get_last_k_paths(path_list, k)\n",
    "print(\"Following model states will be loaded: \")\n",
    "pprint(run_id2model_state_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(ent_list):\n",
    "    ent_memory_set = set()\n",
    "    filtered_ent_list = []\n",
    "    for ent in ent_list:\n",
    "        ent_memory = \"{}\\u2E80{}\\u2E80{}\".format(ent[\"tok_span\"][0], ent[\"tok_span\"][1], ent[\"type\"])\n",
    "        if ent_memory not in ent_memory_set:\n",
    "            filtered_ent_list.append(ent)\n",
    "            ent_memory_set.add(ent_memory)\n",
    "    return filtered_ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_dataloader, ori_test_data):\n",
    "    '''\n",
    "    test_data: if split, it would be samples with subtext\n",
    "    ori_test_data: the original data has not been split, used to get original text here\n",
    "    '''\n",
    "    pred_sample_list = []\n",
    "    for batch_test_data in tqdm(test_dataloader, desc = \"Predicting\"):\n",
    "        \n",
    "        sample_list = batch_test_data[\"sample_list\"]\n",
    "        tok2char_span_list = batch_test_data[\"tok2char_span_list\"]\n",
    "        del batch_test_data[\"sample_list\"]\n",
    "        del batch_test_data[\"tok2char_span_list\"]\n",
    "\n",
    "        for k, v in batch_test_data.items():\n",
    "            if k not in {\"padded_sents\"}:\n",
    "                batch_test_data[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_pred_shaking_outputs = ent_extractor(**batch_test_data)\n",
    "        batch_pred_shaking_tag = (batch_pred_shaking_outputs > 0.).long()\n",
    "\n",
    "        for ind in range(len(sample_list)):\n",
    "            sample = sample_list[ind]\n",
    "            text = sample[\"text\"]\n",
    "            text_id = sample[\"id\"]\n",
    "            tok2char_span = tok2char_span_list[ind]\n",
    "            pred_shaking_tag = batch_pred_shaking_tag[ind]\n",
    "            tok_offset, char_offset = 0, 0\n",
    "            tok_offset, char_offset = (sample[\"tok_offset\"], sample[\"char_offset\"]) if \"char_offset\" in sample else (0, 0)\n",
    "            ent_list = handshaking_tagger.decode_ent(text, \n",
    "                                                     pred_shaking_tag, \n",
    "                                                     tok2char_span, \n",
    "                                                     tok_offset = tok_offset, \n",
    "                                                     char_offset = char_offset)\n",
    "            pred_sample_list.append({\n",
    "                \"text\": text,\n",
    "                \"id\": text_id,\n",
    "                \"entity_list\": ent_list,\n",
    "            })\n",
    "            \n",
    "    # merge\n",
    "    text_id2ent_list = {}\n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        if text_id not in text_id2ent_list:\n",
    "            text_id2ent_list[text_id] = sample[\"entity_list\"]\n",
    "        else:\n",
    "            text_id2ent_list[text_id].extend(sample[\"entity_list\"])\n",
    "\n",
    "    text_id2text = {sample[\"id\"]:sample[\"text\"] for sample in ori_test_data}\n",
    "    merged_pred_sample_list = []\n",
    "    for text_id, ent_list in text_id2ent_list.items():\n",
    "        merged_pred_sample_list.append({\n",
    "            \"id\": text_id,\n",
    "            \"text\": text_id2text[text_id],\n",
    "            \"entity_list\": filter_duplicates(ent_list),\n",
    "        })\n",
    "        \n",
    "    return merged_pred_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_prf(pred_sample_list, gold_test_data, pattern = \"only_head\"):\n",
    "    text_id2gold_n_pred = {}\n",
    "    for sample in gold_test_data:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id] = {\n",
    "            \"gold_entity_list\": sample[\"entity_list\"],\n",
    "        }\n",
    "    \n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id][\"pred_entity_list\"] = sample[\"entity_list\"]\n",
    "\n",
    "    correct_num, pred_num, gold_num = 0, 0, 0\n",
    "    for gold_n_pred in text_id2gold_n_pred.values():\n",
    "        gold_ent_list = gold_n_pred[\"gold_entity_list\"]\n",
    "        pred_ent_list = gold_n_pred[\"pred_entity_list\"] if \"pred_entity_list\" in gold_n_pred else []\n",
    "        if pattern == \"only_head_index\":\n",
    "            gold_ent_set = set([\"{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"type\"]) for ent in gold_ent_list])\n",
    "            pred_ent_set = set([\"{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"type\"]) for ent in pred_ent_list])\n",
    "        elif pattern == \"whole_span\":\n",
    "            gold_ent_set = set([\"{}\\u2E80{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"char_span\"][1], ent[\"type\"]) for ent in gold_ent_list])\n",
    "            pred_ent_set = set([\"{}\\u2E80{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"char_span\"][1], ent[\"type\"]) for ent in pred_ent_list])\n",
    "        elif pattern == \"whole_text\":\n",
    "            gold_ent_set = set([\"{}\\u2E80{}\".format(ent[\"text\"], ent[\"type\"]) for ent in gold_ent_list])\n",
    "            pred_ent_set = set([\"{}\\u2E80{}\".format(ent[\"text\"], ent[\"type\"]) for ent in pred_ent_list])\n",
    "            \n",
    "        for ent_str in pred_ent_set:\n",
    "            if ent_str in gold_ent_set:\n",
    "                correct_num += 1\n",
    "\n",
    "        pred_num += len(pred_ent_set)\n",
    "        gold_num += len(gold_ent_set)\n",
    "#     print((correct_num, pred_num, gold_num))\n",
    "    prf = metrics.get_scores(correct_num, pred_num, gold_num)\n",
    "    return prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "res_dict = {}\n",
    "predict_statistics = {}\n",
    "for file_name, short_data in test_data_dict.items():\n",
    "    ori_test_data = ori_test_data_dict[file_name]\n",
    "    indexed_test_data = data_maker.get_indexed_data(short_data, data_type = \"test\")\n",
    "    test_dataloader = DataLoader(MyDataset(indexed_test_data), \n",
    "                              batch_size = batch_size, \n",
    "                              shuffle = False, \n",
    "                              num_workers = 6,\n",
    "                              drop_last = False,\n",
    "                              collate_fn = lambda data_batch: data_maker.generate_batch(data_batch, data_type = \"test\"),\n",
    "                             )\n",
    "    \n",
    "    # iter all model state dicts\n",
    "    for run_id, model_path_list in run_id2model_state_paths.items():\n",
    "        save_dir4run = os.path.join(save_res_dir, run_id)\n",
    "        if config[\"save_res\"] and not os.path.exists(save_dir4run):\n",
    "            os.makedirs(save_dir4run)\n",
    "            \n",
    "        for model_state_path in model_path_list:\n",
    "            res_num = re.search(\"(\\d+)\", model_state_path.split(\"/\")[-1]).group(1)\n",
    "            save_path = os.path.join(save_dir4run, \"{}_res_{}.json\".format(file_name, res_num))\n",
    "            \n",
    "            if os.path.exists(save_path):\n",
    "                pred_sample_list = [json.loads(line) for line in open(save_path, \"r\", encoding = \"utf-8\")]\n",
    "                print(\"{} already exists, load it directly!\".format(save_path))\n",
    "            else:\n",
    "                # load model state\n",
    "                model_state_dict = torch.load(model_state_path)\n",
    "                # if used paralell train, need to rm prefix \"module.\"\n",
    "                new_model_state_dict = OrderedDict()\n",
    "                for key, v in model_state_dict.items():\n",
    "                    key = re.sub(\"module\\.\", \"\", key)\n",
    "                    new_model_state_dict[key] = v\n",
    "                ent_extractor.load_state_dict(new_model_state_dict)\n",
    "                ent_extractor.eval()\n",
    "                print(\"run_id: {}, model state {} loaded\".format(run_id, model_state_path.split(\"/\")[-1]))\n",
    "\n",
    "                # predict\n",
    "                pred_sample_list = predict(test_dataloader, ori_test_data)\n",
    "            \n",
    "            res_dict[save_path] = pred_sample_list\n",
    "            predict_statistics[save_path] = len([s for s in pred_sample_list if len(s[\"entity_list\"]) > 0])\n",
    "pprint(predict_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "if config[\"score\"]:\n",
    "    filepath2scores = {}\n",
    "    for file_path, pred_samples in res_dict.items():\n",
    "        file_name = re.match(\"(.*?)_res_\\d+.json\", file_path.split(\"/\")[-1]).group(1)\n",
    "        gold_test_data = ori_test_data_dict[file_name]\n",
    "        prf = get_test_prf(pred_samples, gold_test_data, pattern = config[\"correct\"])\n",
    "        filepath2scores[file_path] = prf\n",
    "    print(\"---------------- Results -----------------------\")\n",
    "    pprint(filepath2scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check char span\n",
    "for path, res in res_dict.items():\n",
    "    for sample in tqdm(res, \"check character level span\"):\n",
    "        text = sample[\"text\"]\n",
    "        for ent in sample[\"entity_list\"]:\n",
    "            assert ent[\"text\"] == text[ent[\"char_span\"][0]:ent[\"char_span\"][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "if config[\"save_res\"]:\n",
    "    for path, res in res_dict.items():\n",
    "        with open(path, \"w\", encoding = \"utf-8\") as file_out:\n",
    "            for sample in tqdm(res, desc = \"Output\"):\n",
    "                if len(sample[\"entity_list\"]) == 0:\n",
    "                    continue\n",
    "                json_line = json.dumps(sample, ensure_ascii = False)     \n",
    "                file_out.write(\"{}\\n\".format(json_line))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
