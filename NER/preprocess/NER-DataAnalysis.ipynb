{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, BasicTokenizer, BertTokenizerFast\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data_dir = \"/home/wangyucheng/opt/data/competition/NLP/ner_research\"\n",
    "genia_path = os.path.join(ner_data_dir, \"GENIA_term_3.02\", \"GENIAcorpus3.02.xml\")\n",
    "pretrained_model_home = \"/home/wangyucheng/opt/transformers_models_h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(pretrained_model_home, \"distilbert-base-cased\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path, add_special_tokens = False, do_lower_case = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(genia_path, \"r\", encoding = \"utf-8\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = soup.select(\"set > article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:12<00:00, 161.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "986"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max token nums: article\n",
    "max_tok_num = 0\n",
    "for art in tqdm(article_list):\n",
    "    title = art.select(\"title\")\n",
    "    assert len(title) == 1\n",
    "    title = title[0].get_text()\n",
    "    \n",
    "    abstract = art.select(\"abstract\")\n",
    "    assert len(abstract) == 1\n",
    "    abstract = abstract[0].get_text()\n",
    "    \n",
    "    article = title + \"\\n\" + abstract\n",
    "    tok_num = len(tokenizer.tokenize(article))\n",
    "    max_tok_num = max(max_tok_num, tok_num)\n",
    "max_tok_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:09<00:00, 213.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max token nums: sentence\n",
    "max_tok_num = 0\n",
    "for art in tqdm(article_list):\n",
    "    sens = art.select(\"sentence\")\n",
    "    for sen in sens:\n",
    "        tok_num = len(tokenizer.tokenize(sen.get_text()))\n",
    "        max_tok_num = max(max_tok_num, tok_num)\n",
    "max_tok_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:04<00:00, 425.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max token nums: title\n",
    "max_tok_num = 0\n",
    "for art in tqdm(article_list):\n",
    "    title = art.select(\"title\")\n",
    "    assert len(title) == 1\n",
    "    title = title[0].get_text()\n",
    "\n",
    "    tok_num = len(tokenizer.tokenize(title))\n",
    "    max_tok_num = max(max_tok_num, tok_num)\n",
    "max_tok_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:10<00:00, 196.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max token num of term\n",
    "max_tok_num_term = 0\n",
    "for art in tqdm(article_list):\n",
    "    cons_list = art.select(\"cons\")\n",
    "    for cons in cons_list:\n",
    "        text = cons.get_text()\n",
    "        cons_tokens = tokenizer.tokenize(text)\n",
    "        max_tok_num_term = max(max_tok_num_term, len(cons_tokens))\n",
    "max_tok_num_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:05<00:00, 357.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# 实体边界没有多余空格\n",
    "for art in tqdm(article_list):\n",
    "    cons_list = art.select(\"cons\")\n",
    "    for cons in cons_list:\n",
    "        text = cons.get_text()\n",
    "        if text[0] == \" \" or text[-1] == \" \":\n",
    "            set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:04<00:00, 494.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# 检查是否有多个token对应一个字符，类似于韩文的情况\n",
    "# 没有，那么char span到token span的映射就很简单了，可以先打上char span再打上token span\n",
    "for art in tqdm(article_list):\n",
    "    art_text = art.get_text()\n",
    "    offset_map = tokenizer.encode_plus(art_text, \n",
    "                                       return_offsets_mapping = True, \n",
    "                                       add_special_tokens = False)[\"offset_mapping\"]\n",
    "    for ind, sp in enumerate(offset_map):\n",
    "        if ind == 0:\n",
    "            continue\n",
    "        if sp[0] == offset_map[ind - 1][0] and sp[1] == offset_map[ind - 1][1]:\n",
    "            set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:05<00:00, 351.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# have a look at sem \n",
    "for art in tqdm(article_list):\n",
    "    for cons in art.select(\"cons\"):\n",
    "        if \"sem\" in cons.attrs:\n",
    "            sem_text = cons[\"sem\"]\n",
    "            all_sems = re.findall(\"G#[^\\s()]+\", sem_text)\n",
    "#             if \"BUT\" in sem_text or \"AND\" in sem_text:\n",
    "#                 set_trace()\n",
    "            sem_set = set(all_sems)\n",
    "            if len(sem_set) > 1:\n",
    "                set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 嵌套实体可能不以空格为边界，e.g. deltaNFkappaBdeltaSpl234, deltaNFkappaB, deltaSpl234都是目标实体"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machine_learning)",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
