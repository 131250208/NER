{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "from transformers import BertTokenizerFast\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = \"/home/wangyucheng/workplace/notebook/research/nested_ner\"\n",
    "ori_data_dir = os.path.join(project_root, \"ori_data\")\n",
    "preprocessed_data_dir = os.path.join(project_root, \"preprocessed_data\")\n",
    "exp_name = \"genia\"\n",
    "genia_path = os.path.join(ori_data_dir, \"GENIA_term_3.02\", \"GENIAcorpus3.02.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(genia_path, \"r\", encoding = \"utf-8\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = soup.select(\"set > article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/wangyucheng/opt/transformers_models_h5/bert-base-cased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path, add_special_tokens = False, do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_ind2tok_ind(tok2char_span):\n",
    "    char_num = None\n",
    "    for tok_ind in range(len(tok2char_span) - 1, -1, -1):\n",
    "        if tok2char_span[tok_ind][1] != 0:\n",
    "            char_num = tok2char_span[tok_ind][1]\n",
    "            break\n",
    "    char_ind2tok_ind = [0 for _ in range(char_num)] # 除了空格，其他字符均有对应token\n",
    "    for tok_ind, sp in enumerate(tok2char_span):\n",
    "        for char_ind in range(sp[0], sp[1]):\n",
    "            char_ind2tok_ind[char_ind] = tok_ind\n",
    "    return char_ind2tok_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(article):\n",
    "    '''\n",
    "    article: article tag\n",
    "    return: \n",
    "        article_dict: {\n",
    "            \"id\": medline_id,\n",
    "            \"text\": article_text,\n",
    "            \"entity_list\": [(lex, sem, span), ]\n",
    "        }\n",
    "    '''\n",
    "    article_cp = copy.copy(article) # extract tag fr a copy, avoid removing it from the dom tree\n",
    "    medline_id = article_cp.select_one(\"articleinfo\").extract().select_one(\"bibliomisc\").get_text()\n",
    "    art_text = article_cp.get_text()\n",
    "    article_dict = {\n",
    "        \"id\": medline_id,\n",
    "        \"text\": art_text,\n",
    "    }\n",
    "    \n",
    "    segs = re.sub(\"(<[^>]+>)\", r\"⺀\\1⺀\", str(article_cp)).split(\"⺀\")\n",
    "    # s中某些符号会被转义，在这里要转义回来，如> &lg;\n",
    "    # 如果不转义回来，char pos的计算会错误，如把>算作4个字符（&lg;）\n",
    "    # 因为get_text()会自动转义回去\n",
    "    segs = [html.unescape(s) for s in segs if s != \"\"]\n",
    "    \n",
    "    # count tokens' position\n",
    "    str_w_pos = \"\"\n",
    "    all_char_num = 0\n",
    "    for s in segs:\n",
    "        if re.match(\"<[^>]+>\", s):\n",
    "            str_w_pos += s\n",
    "            continue\n",
    "        char_num = len(s)\n",
    "        char_pos = [str(all_char_num + i) for i in range(char_num)]\n",
    "        if len(char_pos) > 0:\n",
    "            str_w_pos += \" \" + \" \".join(char_pos) + \" \"\n",
    "        all_char_num += char_num\n",
    "#     print(str_w_pos)\n",
    "#     set_trace()\n",
    "    # parse terms' spans\n",
    "    soup = BeautifulSoup(str_w_pos, \"lxml\")\n",
    "    cons_w_pos_list = soup.select(\"cons\")\n",
    "    ori_cons_list = article_cp.select(\"cons\")\n",
    "    assert len(cons_w_pos_list) == len(ori_cons_list) # 检查是否影响了原来的标注\n",
    "\n",
    "    term_list = []\n",
    "    offset_map = tokenizer.encode_plus(art_text, \n",
    "                                       return_offsets_mapping = True, \n",
    "                                       add_special_tokens = False)[\"offset_mapping\"]\n",
    "    char_ind2tok_ind = get_char_ind2tok_ind(offset_map)\n",
    "    for ind, cons in enumerate(cons_w_pos_list):\n",
    "        sem_text = \"[UNK]\" if \"sem\" not in cons.attrs else cons[\"sem\"] \n",
    "        # subtype\n",
    "        subtype = re.search(\"G#[^\\s()]+\", sem_text)\n",
    "        if subtype is not None:\n",
    "            subtype = subtype.group().split(\"#\")[1]\n",
    "        \n",
    "        lex = \"[UNK]\" if \"lex\" not in cons.attrs else re.sub(\"_\", \" \", cons[\"lex\"])  \n",
    "        \n",
    "        # position\n",
    "        pos_num = cons.get_text().strip().split(\" \")\n",
    "        span = (int(pos_num[0]), int(pos_num[-1]) + 1)\n",
    "        \n",
    "        cons_text = ori_cons_list[ind].get_text()\n",
    "        term = {\n",
    "            \"text\": cons_text,\n",
    "            \"lex\": lex,\n",
    "            \"sem\": sem_text,\n",
    "            \"subtype\": subtype,\n",
    "            \"char_span\": span,\n",
    "            \"tok_span\": (char_ind2tok_ind[span[0]], char_ind2tok_ind[span[1] - 1] + 1),\n",
    "        }\n",
    "        term_list.append(term)\n",
    "    article_dict[\"entity_list\"] = term_list\n",
    "    return article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tok2char_span_map(text):\n",
    "    tok2char_span = tokenizer.encode_plus(text, \n",
    "                                           return_offsets_mapping = True, \n",
    "                                           add_special_tokens = False)[\"offset_mapping\"]\n",
    "    return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:44<00:00, 45.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# check spans\n",
    "for art in tqdm(article_list):\n",
    "    art_dict = convert_to_dict(art)\n",
    "    art_text = art_dict[\"text\"]\n",
    "    tok2char_span = get_tok2char_span_map(art_text)\n",
    "    for term in art_dict[\"entity_list\"]:\n",
    "#         # check char span\n",
    "#         char_span = term[\"char_span\"]\n",
    "#         pred_text = art_text[char_span[0]:char_span[1]]\n",
    "#         assert pred_text == term[\"text\"]\n",
    "        \n",
    "        # check tok span\n",
    "        # # voc 里必须加两个token：hypo, mineralo\n",
    "        tok_span = term[\"tok_span\"]\n",
    "        char_span_list = tok2char_span[tok_span[0]:tok_span[1]]\n",
    "        pred_text = art_text[char_span_list[0][0]:char_span_list[-1][1]]\n",
    "        assert pred_text == term[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse(article_dict):\n",
    "    '''\n",
    "    only keep 5 types: RNA, DNA, protein, cell_type, cell_line\n",
    "    '''\n",
    "    new_term_list = []\n",
    "    save_types = {\"RNA\", \"DNA\", \"protein\", \"cell_line\", \"cell_type\"}\n",
    "    \n",
    "    for term in article_dict[\"entity_list\"]:\n",
    "        subtype = term[\"subtype\"]\n",
    "        if subtype is None:\n",
    "            continue\n",
    "        type_ = subtype.split(\"_\")[0] if subtype not in {\"cell_type\", \"cell_line\"} else subtype\n",
    "        if type_ in save_types:\n",
    "            \n",
    "            term[\"type\"] = type_\n",
    "            new_term_list.append(term)\n",
    "    \n",
    "    article_dict[\"entity_list\"] = new_term_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for art in tqdm(article_list):\n",
    "#     art_dict = convert_to_dict(art)\n",
    "#     pprint(art_dict[\"term_list\"])\n",
    "#     print()\n",
    "#     collapse(art_dict)\n",
    "#     for term in art_dict[\"term_list\"]:\n",
    "#         if \"type\" not in term:\n",
    "#             set_trace()\n",
    "#     pprint(art_dict[\"term_list\"])\n",
    "# #     print(\"------------------\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:41<00:00, 48.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert to dict\n",
    "article_dict_list = []\n",
    "for art in tqdm(article_list):\n",
    "    art_dict = convert_to_dict(art)\n",
    "    collapse(art_dict)\n",
    "    article_dict_list.append(art_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 200\n"
     ]
    }
   ],
   "source": [
    "# split into train and eval set\n",
    "train_num = int(len(article_dict_list) * 0.9)\n",
    "train_data, eval_data = article_dict_list[:train_num], article_dict_list[train_num:]\n",
    "print(len(train_data), len(eval_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = os.path.join(preprocessed_data_dir, exp_name)\n",
    "if not os.path.exists(exp_path):\n",
    "    os.mkdir(exp_path)\n",
    "train_save_path = os.path.join(preprocessed_data_dir, exp_name, \"train_data.json\")\n",
    "eval_save_path = os.path.join(preprocessed_data_dir, exp_name, \"eval_data.json\")\n",
    "json.dump(train_data, open(train_save_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "json.dump(eval_data, open(eval_save_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"RNA\", \"DNA\", \"protein\", \"cell_line\", \"cell_type\"]\n",
    "tag_path = os.path.join(preprocessed_data_dir, exp_name, \"tags.json\")\n",
    "json.dump(tags, open(tag_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machine_learning)",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
