experiment_name: genia
run_name: cln_p+bioL

data_home: ../data/biobert-large-cased-pubmed-58k
# # biobert-large-cased-pubmed-58k
# # bert-large-cased
# # bert-base-cased
# # biobert_v1.0_pubmed_pmc

# data file name
train_data: train_data.json
valid_data: valid_data.json
meta: meta.json
word2idx: word2idx.json
char2idx: char2idx.json
 
device_num: 1

wandb: true
path_to_save_model: ../model_state

hyper_parameters:
 # >>>>>>>>>Preprocessing>>>>>>>>>>>
#  token_type: subword
 max_seq_len: 128
 pred_max_seq_len: 128
 sliding_len: 20
 pred_sliding_len: 20
 
 # >>>>>>>>Train config>>>>>>>>>>>>
 batch_size: 36
 parallel: false
 epochs: 100
 lr: 2e-5
 seed: 2333
 log_interval: 10
 ## learning schedule
 ### CAWR
 scheduler: CAWR # Step
 T_mult: 1
 rewarm_epoch_num: 3
#  ### StepLR
#  scheduler: Step
#  decay_rate: 0.99
#  decay_steps: 100
 
 # >>>>>>>>Model config>>>>>>>>>>>>>
 ## char encoder
 use_char_encoder: true
 char_encoder_config:
  emb_dim: 64
  emb_dropout: 0.1
  bilstm_layers: 
   - 1
   - 1
  bilstm_hidden_size: 
   - 64
   - 128
  bilstm_dropout:
   - 0.
   - 0.05
   - 0.
 ## bert: subword encoder
 use_bert: true
 bert_config:
  path: ../../pretrained_models/biobert-large-cased-pubmed-58k # biobert_v1.0_pubmed_pmc
  use_last_k_layers: 1
  finetune: true
 ## word encoder
 use_word_encoder: true
 word_encoder_config:
  emb_key: pubmed # pubmed, glove
  emb_dropout: 0.1
  bilstm_layers: 
   - 1
   - 1
  bilstm_hidden_size: 
   - 300
   - 600
  bilstm_dropout:
   - 0.
   - 0.05
   - 0.
  freeze_word_emb: false
 ## flair
 use_flair: true
 flair_embedding_ids:
  - pubmed-forward
  - pubmed-backward
 ## handshaking kernel
 handshaking_kernel_config:
  visual_field: -1 # -1: use recommended value
  shaking_type: cat_plus
  context_type: lstm
 ## encoding fc
 enc_hidden_size: 1024
 activate_enc_fc: false
 
# when to save the model state dict
f1_2_save: 0
# whether train from scratch
fr_scratch: true
# note 
note: start from scratch;
# if not fr scratch, set a model_state_dict
model_state_dict_path: stake
