{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "from pprint import pprint\n",
    "import unicodedata\n",
    "from transformers import AutoModel, BasicTokenizer, BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import time\n",
    "from ner_common.utils import Preprocessor\n",
    "from tplinker_ner import (HandshakingTaggingScheme,\n",
    "                          DataMaker, \n",
    "                          TPLinkerNER,\n",
    "                          Metrics)\n",
    "import wandb\n",
    "import yaml\n",
    "from glove import Glove\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "config = yaml.load(open(\"eval_config.yaml\", \"r\"), Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"device_num\"])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = config[\"data_home\"]\n",
    "exp_name = config[\"exp_name\"]\n",
    "test_data_path = os.path.join(data_home, exp_name, config[\"test_data\"])\n",
    "meta_path = os.path.join(data_home, exp_name, config[\"meta\"])\n",
    "batch_size = config[\"batch_size\"]\n",
    "encoder_path = config[\"bert_path\"]\n",
    "visual_field = config[\"visual_field\"]\n",
    "use_last_k_layers_hiddens = config[\"use_last_k_layers_hiddens\"]\n",
    "add_bilstm_on_the_top = config[\"add_bilstm_on_the_top\"]\n",
    "bilstm_layers = config[\"bilstm_layers\"]\n",
    "bilstm_dropout = config[\"bilstm_dropout\"]\n",
    "save_res_dir = os.path.join(config[\"save_res_dir\"], exp_name)\n",
    "\n",
    "# for reproductivity\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path_dict = {}\n",
    "for path in glob.glob(test_data_path):\n",
    "    file_name = re.search(\"(.*?)\\.json\", path.split(\"/\")[-1]).group(1)\n",
    "    test_data_path_dict[file_name] = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict = {}\n",
    "for file_name, path in test_data_path_dict.items():\n",
    "    test_data_dict[file_name] = json.load(open(path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(encoder_path, add_special_tokens = False, do_lower_case = False)\n",
    "    tokenize = tokenizer.tokenize\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "# elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "#     tokenize = lambda text: text.split(\" \")\n",
    "#     def get_tok2char_span_map(text):\n",
    "#         tokens = text.split(\" \")\n",
    "#         tok2char_span = []\n",
    "#         char_num = 0\n",
    "#         for tok in tokens:\n",
    "#             tok2char_span.append((char_num, char_num + len(tok)))\n",
    "#             char_num += len(tok) + 1 # +1: whitespace\n",
    "#         return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(tokenize_func = tokenize, \n",
    "                            get_tok2char_span_map_func = get_tok2char_span_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate the max token number: 100%|██████████| 1855/1855 [00:00<00:00, 5671.75it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "for data in list(test_data_dict.values()):\n",
    "    all_data.extend(data)\n",
    "    \n",
    "max_tok_num = 0\n",
    "for sample in tqdm(all_data, desc = \"Calculate the max token number\"):\n",
    "    tokens = tokenize(sample[\"text\"])\n",
    "    max_tok_num = max(len(tokens), max_tok_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting:  10%|█         | 187/1855 [00:00<00:00, 1867.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_tok_num: 196, lagger than max_test_seq_len: 128, test data will be split!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting: 100%|██████████| 1855/1855 [00:00<00:00, 2837.89it/s]\n"
     ]
    }
   ],
   "source": [
    "split_test_data = False\n",
    "if max_tok_num > config[\"max_test_seq_len\"]:\n",
    "    split_test_data = True\n",
    "    print(\"max_tok_num: {}, lagger than max_test_seq_len: {}, test data will be split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "else:\n",
    "    print(\"max_tok_num: {}, less than or equal to max_test_seq_len: {}, no need to split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "max_seq_len = min(max_tok_num, config[\"max_test_seq_len\"]) \n",
    "\n",
    "if config[\"force_split\"]:\n",
    "    split_test_data = True\n",
    "    print(\"force to split the test dataset!\")    \n",
    "\n",
    "ori_test_data_dict = copy.deepcopy(test_data_dict)\n",
    "if split_test_data:\n",
    "    test_data_dict = {}\n",
    "    for file_name, data in ori_test_data_dict.items():\n",
    "        test_data_dict[file_name] = preprocessor.split_into_short_samples(data, \n",
    "                                                                          max_seq_len, \n",
    "                                                                          sliding_len = config[\"sliding_len\"], \n",
    "                                                                          encoder = config[\"encoder\"], \n",
    "                                                                          data_type = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data: 1862\n"
     ]
    }
   ],
   "source": [
    "for filename, short_data in test_data_dict.items():\n",
    "    print(\"{}: {}\".format(filename, len(short_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder(Tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended visual_field is greater than current visual_field, reset to rec val: 45\n"
     ]
    }
   ],
   "source": [
    "meta = json.load(open(meta_path, \"r\", encoding = \"utf-8\"))\n",
    "tags = meta[\"tags\"]\n",
    "if meta[\"visual_field_rec\"] > visual_field:\n",
    "    visual_field = meta[\"visual_field_rec\"]\n",
    "    print(\"Recommended visual_field is greater than current visual_field, reset to rec val: {}\".format(visual_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "handshaking_tagger = HandshakingTaggingScheme(tags, max_seq_len, visual_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    data_maker = DataMaker(handshaking_tagger, tokenizer)\n",
    "    \n",
    "# elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "#     token2idx_path = os.path.join(*config[\"token2idx_path\"])\n",
    "#     token2idx = json.load(open(token2idx_path, \"r\", encoding = \"utf-8\"))\n",
    "#     idx2token = {idx:tok for tok, idx in token2idx.items()}\n",
    "#     def text2indices(text, max_seq_len):\n",
    "#         input_ids = []\n",
    "#         tokens = text.split(\" \")\n",
    "#         for tok in tokens:\n",
    "#             if tok not in token2idx:\n",
    "#                 input_ids.append(token2idx['<UNK>'])\n",
    "#             else:\n",
    "#                 input_ids.append(token2idx[tok])\n",
    "#         if len(input_ids) < max_seq_len:\n",
    "#             input_ids.extend([token2idx['<PAD>']] * (max_seq_len - len(input_ids)))\n",
    "#         input_ids = torch.tensor(input_ids[:max_seq_len])\n",
    "#         return input_ids\n",
    "#     data_maker = DataMaker4BiLSTM(text2indices, get_tok2char_span_map, handshaking_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    encoder = AutoModel.from_pretrained(encoder_path)\n",
    "    hidden_size = encoder.config.hidden_size\n",
    "    fake_input = torch.zeros([batch_size, max_seq_len, hidden_size]).to(device)\n",
    "    shaking_type = config[\"shaking_type\"]\n",
    "    ent_extractor = TPLinkerNER(encoder, use_last_k_layers_hiddens, add_bilstm_on_the_top, bilstm_layers, bilstm_dropout, len(tags), fake_input, shaking_type, visual_field)\n",
    "     \n",
    "# elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "#     glove = Glove()\n",
    "#     glove = glove.load(config[\"pretrained_word_embedding_path\"])\n",
    "    \n",
    "#     # prepare embedding matrix\n",
    "#     word_embedding_init_matrix = np.random.normal(-1, 1, size=(len(token2idx), config[\"word_embedding_dim\"]))\n",
    "#     count_in = 0\n",
    "\n",
    "#     # 在预训练词向量中的用该预训练向量\n",
    "#     # 不在预训练集里的用随机向量\n",
    "#     for ind, tok in tqdm(idx2token.items(), desc=\"Embedding matrix initializing...\"):\n",
    "#         if tok in glove.dictionary:\n",
    "#             count_in += 1\n",
    "#             word_embedding_init_matrix[ind] = glove.word_vectors[glove.dictionary[tok]]\n",
    "\n",
    "#     print(\"{:.4f} tokens are in the pretrain word embedding matrix\".format(count_in / len(idx2token))) # 命中预训练词向量的比例\n",
    "#     word_embedding_init_matrix = torch.FloatTensor(word_embedding_init_matrix)\n",
    "    \n",
    "#     fake_inputs = torch.zeros([config[\"test_batch_size\"], max_seq_len, config[\"dec_hidden_size\"]])\n",
    "#     rel_extractor = TPLinkerBiLSTM(word_embedding_init_matrix, \n",
    "#                                    config[\"emb_dropout\"], \n",
    "#                                    config[\"enc_hidden_size\"], \n",
    "#                                    config[\"dec_hidden_size\"],\n",
    "#                                    config[\"rnn_dropout\"],\n",
    "#                                    len(rel2id), \n",
    "#                                    fake_inputs,\n",
    "#                                    config[\"shaking_type\"],\n",
    "#                                   )\n",
    "ent_extractor = ent_extractor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics(handshaking_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model state paths\n",
    "model_state_dir = config[\"model_state_dict_dir\"]\n",
    "target_run_ids = set(config[\"run_ids\"])\n",
    "run_id2model_state_paths = {}\n",
    "for root, dirs, files in os.walk(model_state_dir):\n",
    "    for file_name in files:\n",
    "        run_id = root.split(\"-\")[-1]\n",
    "        if re.match(\".*model_state.*\\.pt\", file_name) and run_id in target_run_ids:\n",
    "            if run_id not in run_id2model_state_paths:\n",
    "                run_id2model_state_paths[run_id] = []\n",
    "            model_state_path = os.path.join(root, file_name)\n",
    "            run_id2model_state_paths[run_id].append(model_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_k_paths(path_list, k):\n",
    "    path_list = sorted(path_list, key = lambda x: int(re.search(\"(\\d+)\", x.split(\"/\")[-1]).group(1)))\n",
    "#     pprint(path_list)\n",
    "    return path_list[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only last k models\n",
    "k = config[\"last_k_model\"]\n",
    "for run_id, path_list in run_id2model_state_paths.items():\n",
    "    run_id2model_state_paths[run_id] = get_last_k_paths(path_list, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(ent_list):\n",
    "    ent_memory_set = set()\n",
    "    filtered_ent_list = []\n",
    "    for ent in ent_list:\n",
    "        ent_memory = \"{}\\u2E80{}\\u2E80{}\".format(ent[\"tok_span\"][0], ent[\"tok_span\"][1], ent[\"type\"])\n",
    "        if ent_memory not in ent_memory_set:\n",
    "            filtered_ent_list.append(ent)\n",
    "            ent_memory_set.add(ent_memory)\n",
    "    return filtered_ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, ori_test_data):\n",
    "    '''\n",
    "    test_data: if split, it would be samples with subtext\n",
    "    ori_test_data: the original data has not been split, used to get original text here\n",
    "    '''\n",
    "    indexed_test_data = data_maker.get_indexed_data(test_data, max_seq_len, data_type = \"test\") # fill up to max_seq_len\n",
    "    test_dataloader = DataLoader(MyDataset(indexed_test_data), \n",
    "                              batch_size = batch_size, \n",
    "                              shuffle = False, \n",
    "                              num_workers = 6,\n",
    "                              drop_last = False,\n",
    "                              collate_fn = lambda data_batch: data_maker.generate_batch(data_batch, data_type = \"test\"),\n",
    "                             )\n",
    "    \n",
    "    pred_sample_list = []\n",
    "    for batch_test_data in tqdm(test_dataloader, desc = \"Predicting\"):\n",
    "        if config[\"encoder\"] == \"BERT\":\n",
    "            sample_list, batch_input_ids, \\\n",
    "            batch_attention_mask, batch_token_type_ids, \\\n",
    "            tok2char_span_list, _ = batch_test_data\n",
    "\n",
    "            batch_input_ids, \\\n",
    "            batch_attention_mask, \\\n",
    "            batch_token_type_ids = (batch_input_ids.to(device), \n",
    "                                      batch_attention_mask.to(device), \n",
    "                                      batch_token_type_ids.to(device))\n",
    "\n",
    "#         elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "#             text_id_list, text_list, batch_input_ids, tok2char_span_list = batch_test_data\n",
    "#             batch_input_ids = batch_input_ids.to(device)\n",
    "     \n",
    "        with torch.no_grad():\n",
    "            if config[\"encoder\"] == \"BERT\":\n",
    "                batch_pred_shaking_outputs = ent_extractor(batch_input_ids, \n",
    "                                                          batch_attention_mask, \n",
    "                                                          batch_token_type_ids, \n",
    "                                                         )\n",
    "#             elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "#                 batch_pred_shaking_outputs = ent_extractor(batch_input_ids)\n",
    "\n",
    "        batch_pred_shaking_tag = (batch_pred_shaking_outputs > 0.).long()\n",
    "\n",
    "        for ind in range(len(sample_list)):\n",
    "            sample = sample_list[ind]\n",
    "            text = sample[\"text\"]\n",
    "            text_id = sample[\"id\"]\n",
    "            tok2char_span = tok2char_span_list[ind]\n",
    "            pred_shaking_tag = batch_pred_shaking_tag[ind]\n",
    "            tok_offset, char_offset = 0, 0\n",
    "            if split_test_data:\n",
    "                tok_offset, char_offset = sample[\"tok_offset\"], sample[\"char_offset\"]\n",
    "            ent_list = handshaking_tagger.decode_ent(text, \n",
    "                                                     pred_shaking_tag, \n",
    "                                                     tok2char_span, \n",
    "                                                     tok_offset = tok_offset, \n",
    "                                                     char_offset = char_offset)\n",
    "            pred_sample_list.append({\n",
    "                \"text\": text,\n",
    "                \"id\": text_id,\n",
    "                \"entity_list\": ent_list,\n",
    "            })\n",
    "            \n",
    "    # merge\n",
    "    text_id2ent_list = {}\n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        if text_id not in text_id2ent_list:\n",
    "            text_id2ent_list[text_id] = sample[\"entity_list\"]\n",
    "        else:\n",
    "            text_id2ent_list[text_id].extend(sample[\"entity_list\"])\n",
    "\n",
    "    text_id2text = {sample[\"id\"]:sample[\"text\"] for sample in ori_test_data}\n",
    "    merged_pred_sample_list = []\n",
    "    for text_id, ent_list in text_id2ent_list.items():\n",
    "        merged_pred_sample_list.append({\n",
    "            \"id\": text_id,\n",
    "            \"text\": text_id2text[text_id],\n",
    "            \"entity_list\": filter_duplicates(ent_list),\n",
    "        })\n",
    "        \n",
    "    return merged_pred_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_prf(pred_sample_list, gold_test_data, pattern = \"only_head\"):\n",
    "    text_id2gold_n_pred = {}\n",
    "    for sample in gold_test_data:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id] = {\n",
    "            \"gold_entity_list\": sample[\"entity_list\"],\n",
    "        }\n",
    "    \n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id][\"pred_entity_list\"] = sample[\"entity_list\"]\n",
    "\n",
    "    correct_num, pred_num, gold_num = 0, 0, 0\n",
    "    for gold_n_pred in text_id2gold_n_pred.values():\n",
    "        gold_ent_list = gold_n_pred[\"gold_entity_list\"]\n",
    "        pred_ent_list = gold_n_pred[\"pred_entity_list\"] if \"pred_entity_list\" in gold_n_pred else []\n",
    "        if pattern == \"only_head_index\":\n",
    "            gold_ent_set = set([\"{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"type\"]) for ent in gold_ent_list])\n",
    "            pred_ent_set = set([\"{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"type\"]) for ent in pred_ent_list])\n",
    "        elif pattern == \"whole_span\":\n",
    "            gold_ent_set = set([\"{}\\u2E80{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"char_span\"][1], ent[\"type\"]) for ent in gold_ent_list])\n",
    "            pred_ent_set = set([\"{}\\u2E80{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"char_span\"][1], ent[\"type\"]) for ent in pred_ent_list])\n",
    "        elif pattern == \"whole_text\":\n",
    "            gold_ent_set = set([\"{}\\u2E80{}\".format(ent[\"text\"], ent[\"type\"]) for ent in gold_ent_list])\n",
    "            pred_ent_set = set([\"{}\\u2E80{}\".format(ent[\"text\"], ent[\"type\"]) for ent in pred_ent_list])\n",
    "            \n",
    "        for ent_str in pred_ent_set:\n",
    "            if ent_str in gold_ent_set:\n",
    "                correct_num += 1\n",
    "\n",
    "        pred_num += len(pred_ent_set)\n",
    "        gold_num += len(gold_ent_set)\n",
    "#     print((correct_num, pred_num, gold_num))\n",
    "    prf = metrics.get_scores(correct_num, pred_num, gold_num)\n",
    "    return prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data:  14%|█▍        | 260/1862 [00:00<00:00, 2599.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_0.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 3194.54it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:18<00:00,  3.26it/s]\n",
      "Generate indexed data:  23%|██▎       | 433/1862 [00:00<00:00, 4322.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_1.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4308.69it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:17<00:00,  3.30it/s]\n",
      "Generate indexed data:  22%|██▏       | 416/1862 [00:00<00:00, 4158.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_2.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4322.98it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:16<00:00,  3.67it/s]\n",
      "Generate indexed data:   2%|▏         | 42/1862 [00:00<00:07, 235.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_3.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 3185.81it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:18<00:00,  3.18it/s]\n",
      "Generate indexed data:  21%|██        | 393/1862 [00:00<00:00, 3927.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_4.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4268.86it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:18<00:00,  3.23it/s]\n",
      "Generate indexed data:  18%|█▊        | 337/1862 [00:00<00:00, 3369.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_5.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4200.51it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:18<00:00,  3.27it/s]\n",
      "Generate indexed data:  24%|██▍       | 449/1862 [00:00<00:00, 4488.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_6.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4306.36it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:17<00:00,  3.42it/s]\n",
      "Generate indexed data:  19%|█▉        | 358/1862 [00:00<00:00, 3573.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_7.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4071.90it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:16<00:00,  3.63it/s]\n",
      "Generate indexed data:  23%|██▎       | 424/1862 [00:00<00:00, 4234.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_8.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 2999.92it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:17<00:00,  3.33it/s]\n",
      "Generate indexed data:  19%|█▉        | 360/1862 [00:00<00:00, 3588.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_9.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4292.38it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:15<00:00,  3.80it/s]\n",
      "Generate indexed data:  18%|█▊        | 338/1862 [00:00<00:00, 3375.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_10.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4107.80it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:15<00:00,  3.89it/s]\n",
      "Generate indexed data:  24%|██▍       | 449/1862 [00:00<00:00, 4486.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_11.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 3123.73it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:15<00:00,  3.81it/s]\n",
      "Generate indexed data:  24%|██▍       | 444/1862 [00:00<00:00, 4436.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_12.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4511.16it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:17<00:00,  3.28it/s]\n",
      "Generate indexed data:  23%|██▎       | 434/1862 [00:00<00:00, 4331.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_13.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4264.67it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:17<00:00,  3.45it/s]\n",
      "Generate indexed data:  22%|██▏       | 415/1862 [00:00<00:00, 4144.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_14.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4268.94it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:19<00:00,  3.03it/s]\n",
      "Generate indexed data:  18%|█▊        | 334/1862 [00:00<00:00, 3335.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_15.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 3964.63it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:17<00:00,  3.34it/s]\n",
      "Generate indexed data:  23%|██▎       | 435/1862 [00:00<00:00, 4344.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_16.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 3502.84it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:15<00:00,  3.73it/s]\n",
      "Generate indexed data:  22%|██▏       | 417/1862 [00:00<00:00, 4165.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2hfxxe3c, model state model_state_dict_17.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 1862/1862 [00:00<00:00, 4166.77it/s]\n",
      "Predicting: 100%|██████████| 59/59 [00:16<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'../results/genia/2hfxxe3c/test_data_res_0.json': 0,\n",
      " '../results/genia/2hfxxe3c/test_data_res_1.json': 888,\n",
      " '../results/genia/2hfxxe3c/test_data_res_10.json': 1661,\n",
      " '../results/genia/2hfxxe3c/test_data_res_11.json': 1653,\n",
      " '../results/genia/2hfxxe3c/test_data_res_12.json': 1653,\n",
      " '../results/genia/2hfxxe3c/test_data_res_13.json': 1647,\n",
      " '../results/genia/2hfxxe3c/test_data_res_14.json': 1654,\n",
      " '../results/genia/2hfxxe3c/test_data_res_15.json': 1663,\n",
      " '../results/genia/2hfxxe3c/test_data_res_16.json': 1651,\n",
      " '../results/genia/2hfxxe3c/test_data_res_17.json': 1658,\n",
      " '../results/genia/2hfxxe3c/test_data_res_2.json': 1246,\n",
      " '../results/genia/2hfxxe3c/test_data_res_3.json': 1549,\n",
      " '../results/genia/2hfxxe3c/test_data_res_4.json': 1564,\n",
      " '../results/genia/2hfxxe3c/test_data_res_5.json': 1640,\n",
      " '../results/genia/2hfxxe3c/test_data_res_6.json': 1642,\n",
      " '../results/genia/2hfxxe3c/test_data_res_7.json': 1666,\n",
      " '../results/genia/2hfxxe3c/test_data_res_8.json': 1653,\n",
      " '../results/genia/2hfxxe3c/test_data_res_9.json': 1661}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "res_dict = {}\n",
    "predict_statistics = {}\n",
    "for file_name, short_data in test_data_dict.items():\n",
    "    ori_test_data = ori_test_data_dict[file_name]\n",
    "    for run_id, model_path_list in run_id2model_state_paths.items():\n",
    "        save_dir4run = os.path.join(save_res_dir, run_id)\n",
    "        if config[\"save_res\"] and not os.path.exists(save_dir4run):\n",
    "            os.makedirs(save_dir4run)\n",
    "            \n",
    "        for model_state_path in model_path_list:\n",
    "            res_num = re.search(\"(\\d+)\", model_state_path.split(\"/\")[-1]).group(1)\n",
    "            save_path = os.path.join(save_dir4run, \"{}_res_{}.json\".format(file_name, res_num))\n",
    "            \n",
    "            if os.path.exists(save_path):\n",
    "                pred_sample_list = [json.loads(line) for line in open(save_path, \"r\", encoding = \"utf-8\")]\n",
    "                print(\"{} already exists, load it directly!\".format(save_path))\n",
    "            else:\n",
    "                # load model state\n",
    "                ent_extractor.load_state_dict(torch.load(model_state_path))\n",
    "                ent_extractor.eval()\n",
    "                print(\"run_id: {}, model state {} loaded\".format(run_id, model_state_path.split(\"/\")[-1]))\n",
    "\n",
    "                # predict\n",
    "                pred_sample_list = predict(short_data, ori_test_data)\n",
    "            \n",
    "            res_dict[save_path] = pred_sample_list\n",
    "            predict_statistics[save_path] = len([s for s in pred_sample_list if len(s[\"entity_list\"]) > 0])\n",
    "pprint(predict_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Results -----------------------\n",
      "{'../results/genia/2hfxxe3c/test_data_res_0.json': (0.0, 0.0, 0.0),\n",
      " '../results/genia/2hfxxe3c/test_data_res_1.json': (0.5729411764705545,\n",
      "                                                    0.17405289492494327,\n",
      "                                                    0.26699561399933774),\n",
      " '../results/genia/2hfxxe3c/test_data_res_10.json': (0.7276641550053691,\n",
      "                                                     0.7248034310221457,\n",
      "                                                     0.7262309757780981),\n",
      " '../results/genia/2hfxxe3c/test_data_res_11.json': (0.7472710453283858,\n",
      "                                                     0.7217655468191436,\n",
      "                                                     0.7342968820516288),\n",
      " '../results/genia/2hfxxe3c/test_data_res_12.json': (0.7374152088539675,\n",
      "                                                     0.7382058613295079,\n",
      "                                                     0.7378103232219998),\n",
      " '../results/genia/2hfxxe3c/test_data_res_13.json': (0.7352889513437426,\n",
      "                                                     0.7480343102215735,\n",
      "                                                     0.7416068738980817),\n",
      " '../results/genia/2hfxxe3c/test_data_res_14.json': (0.7474153297682576,\n",
      "                                                     0.7492852037169273,\n",
      "                                                     0.7483490986471132),\n",
      " '../results/genia/2hfxxe3c/test_data_res_15.json': (0.7517092479308969,\n",
      "                                                     0.7466047176554549,\n",
      "                                                     0.7491482875598132),\n",
      " '../results/genia/2hfxxe3c/test_data_res_16.json': (0.7593286913051105,\n",
      "                                                     0.760007147962817,\n",
      "                                                     0.7596677681021701),\n",
      " '../results/genia/2hfxxe3c/test_data_res_17.json': (0.7604822746085881,\n",
      "                                                     0.7551822730521667,\n",
      "                                                     0.7578230072126071),\n",
      " '../results/genia/2hfxxe3c/test_data_res_2.json': (0.5623707787732405,\n",
      "                                                    0.29163688348820066,\n",
      "                                                    0.38409037416071135),\n",
      " '../results/genia/2hfxxe3c/test_data_res_3.json': (0.6345266857017212,\n",
      "                                                    0.5162616154395905,\n",
      "                                                    0.5693171740563401),\n",
      " '../results/genia/2hfxxe3c/test_data_res_4.json': (0.6498272884283106,\n",
      "                                                    0.5378842030021348,\n",
      "                                                    0.5885803675687353),\n",
      " '../results/genia/2hfxxe3c/test_data_res_5.json': (0.667974588938702,\n",
      "                                                    0.6388491779842631,\n",
      "                                                    0.6530873218352887),\n",
      " '../results/genia/2hfxxe3c/test_data_res_6.json': (0.6740727139184599,\n",
      "                                                    0.6560042887776866,\n",
      "                                                    0.6649157760775103),\n",
      " '../results/genia/2hfxxe3c/test_data_res_7.json': (0.6845310596833011,\n",
      "                                                    0.7030021443888366,\n",
      "                                                    0.6936436568308922),\n",
      " '../results/genia/2hfxxe3c/test_data_res_8.json': (0.7021125510385106,\n",
      "                                                    0.706754824874898,\n",
      "                                                    0.7044260396685738),\n",
      " '../results/genia/2hfxxe3c/test_data_res_9.json': (0.7191633893457147,\n",
      "                                                    0.7189063616869064,\n",
      "                                                    0.7190348524969039)}\n"
     ]
    }
   ],
   "source": [
    "# score\n",
    "if config[\"score\"]:\n",
    "    filepath2scores = {}\n",
    "    for file_path, pred_samples in res_dict.items():\n",
    "        file_name = re.match(\"(.*?)_res_\\d+.json\", file_path.split(\"/\")[-1]).group(1)\n",
    "        gold_test_data = ori_test_data_dict[file_name]\n",
    "        prf = get_test_prf(pred_samples, gold_test_data, pattern = config[\"correct\"])\n",
    "        filepath2scores[file_path] = prf\n",
    "    print(\"---------------- Results -----------------------\")\n",
    "    pprint(filepath2scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 630300.87it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 482956.79it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 316252.09it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 237795.59it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 242751.67it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 237208.35it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 231037.95it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 250287.39it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 264757.68it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 210174.07it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 213824.55it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 281807.89it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 278170.68it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 287737.94it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 295890.24it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 304470.30it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 316909.04it/s]\n",
      "check character level span: 100%|██████████| 1855/1855 [00:00<00:00, 331816.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "for path, res in res_dict.items():\n",
    "    for sample in tqdm(res, \"check character level span\"):\n",
    "        text = sample[\"text\"]\n",
    "        for ent in sample[\"entity_list\"]:\n",
    "            assert ent[\"text\"] == text[ent[\"char_span\"][0]:ent[\"char_span\"][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "if config[\"save_res\"]:\n",
    "    for path, res in res_dict.items():\n",
    "        with open(path, \"w\", encoding = \"utf-8\") as file_out:\n",
    "            for sample in tqdm(res, desc = \"Output\"):\n",
    "                if len(sample[\"entity_list\"]) == 0:\n",
    "                    continue\n",
    "                json_line = json.dumps(sample, ensure_ascii = False)     \n",
    "                file_out.write(\"{}\\n\".format(json_line))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
