{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "import yaml\n",
    "import os\n",
    "config = yaml.load(open(\"eval_config.yaml\", \"r\"), Loader = yaml.FullLoader)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"device_num\"])\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "from pprint import pprint\n",
    "import unicodedata\n",
    "from transformers import AutoModel, BasicTokenizer, BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import time\n",
    "from ner_common.utils import Preprocessor, WordTokenizer\n",
    "from tplinker_ner import (HandshakingTaggingScheme,\n",
    "                          DataMaker, \n",
    "                          TPLinkerNER,\n",
    "                          Metrics)\n",
    "import wandb\n",
    "from glove import Glove\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count(), \"GPUs are available\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# meta_path = os.path.join(data_home, exp_name, config[\"meta\"])\n",
    "# batch_size = config[\"batch_size\"]\n",
    "# encoder_path = config[\"bert_path\"]\n",
    "# visual_field = config[\"visual_field\"]\n",
    "# use_last_k_layers_hiddens = config[\"use_last_k_layers_hiddens\"]\n",
    "# add_bilstm_on_the_top = config[\"add_bilstm_on_the_top\"]\n",
    "# bilstm_layers = config[\"bilstm_layers\"]\n",
    "# bilstm_dropout = config[\"bilstm_dropout\"]\n",
    "\n",
    "# >>>>>>>>>data path>>>>>>>>>>>>>>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproductivity\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# >>>>>>>>predication config>>>>>>\n",
    "max_seq_len = config[\"max_seq_len\"]\n",
    "sliding_len = config[\"sliding_len\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "\n",
    "# >>>>>>>>>>model config>>>>>>>>>>>>\n",
    "## char encoder\n",
    "char_encoder_config = config[\"char_encoder_config\"] if config[\"use_char_encoder\"] else None\n",
    "\n",
    "## bert encoder\n",
    "bert_config = config[\"bert_config\"] if config[\"use_bert\"] else None\n",
    "use_bert = config[\"use_bert\"]\n",
    "\n",
    "## word encoder\n",
    "word_encoder_config = config[\"word_encoder_config\"] if config[\"use_word_encoder\"] else None\n",
    "\n",
    "## flair config\n",
    "flair_config = {\n",
    "    \"embedding_ids\": config[\"flair_embedding_ids\"],\n",
    "} if config[\"use_flair\"] else None\n",
    "\n",
    "## handshaking_kernel\n",
    "handshaking_kernel_config = config[\"handshaking_kernel_config\"]\n",
    "visual_field = handshaking_kernel_config[\"visual_field\"]\n",
    "\n",
    "## encoding fc\n",
    "enc_hidden_size = config[\"enc_hidden_size\"]\n",
    "activate_enc_fc = config[\"activate_enc_fc\"]\n",
    "\n",
    "# >>>>>>>>>data path>>>>>>>>>>>>>>\n",
    "data_home = config[\"data_home\"]\n",
    "exp_name = config[\"exp_name\"]\n",
    "meta_path = os.path.join(data_home, exp_name, config[\"meta\"])\n",
    "save_res_dir = os.path.join(config[\"save_res_dir\"], exp_name)\n",
    "test_data_path = os.path.join(data_home, exp_name, config[\"test_data\"])\n",
    "word2idx_path = os.path.join(data_home, exp_name, config[\"word2idx\"])\n",
    "char2idx_path = os.path.join(data_home, exp_name, config[\"char2idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path_dict = {}\n",
    "for path in glob.glob(test_data_path):\n",
    "    file_name = re.search(\"(.*?)\\.json\", path.split(\"/\")[-1]).group(1)\n",
    "    test_data_path_dict[file_name] = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict = {}\n",
    "for file_name, path in test_data_path_dict.items():\n",
    "    test_data_dict[file_name] = json.load(open(path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tokenizers\n",
    "if use_bert:\n",
    "    bert_tokenizer = BertTokenizerFast.from_pretrained(bert_config[\"path\"], add_special_tokens = False, do_lower_case = False)\n",
    "word2idx = json.load(open(word2idx_path, \"r\", encoding = \"utf-8\"))\n",
    "word_tokenizer = WordTokenizer(word2idx)\n",
    "\n",
    "# preprocessor\n",
    "tokenizer4preprocess = bert_tokenizer if use_bert else word_tokenizer\n",
    "preprocessor = Preprocessor(tokenizer4preprocess, use_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, max_seq_len, sliding_len, data_name = \"train\"):\n",
    "    '''\n",
    "    split into short texts\n",
    "    '''\n",
    "    max_tok_num = 0\n",
    "    for sample in tqdm(data, \"calculating the max token number of {}\".format(data_name)):\n",
    "        text = sample[\"text\"]\n",
    "        tokens = preprocessor.tokenize(text)\n",
    "        max_tok_num = max(max_tok_num, len(tokens))\n",
    "    print(\"max token number of {}: {}\".format(data_name, max_tok_num))\n",
    "    \n",
    "    if max_tok_num > max_seq_len:\n",
    "        print(\"max token number of {} is greater than the setting, need to split!, max_seq_len of {} is {}\".format(data_name, data_name, max_seq_len))\n",
    "        short_data = preprocessor.split_into_short_samples(data, \n",
    "                                          max_seq_len, \n",
    "                                          sliding_len = sliding_len, \n",
    "                                          data_type = \"test\")\n",
    "    else:\n",
    "        short_data = data\n",
    "        max_seq_len = max_tok_num\n",
    "        print(\"max token number of {} is less than the setting, no need to split! max_seq_len of {} is reset to {}.\".format(data_name, data_name, max_tok_num))\n",
    "    return short_data, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate the max token number: 100%|██████████| 645/645 [00:00<00:00, 10053.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# all_data = []\n",
    "# for data in list(test_data_dict.values()):\n",
    "#     all_data.extend(data)\n",
    "    \n",
    "# max_tok_num = 0\n",
    "# for sample in tqdm(all_data, desc = \"Calculate the max token number\"):\n",
    "#     tokens = tokenize(sample[\"text\"])\n",
    "#     max_tok_num = max(len(tokens), max_tok_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting: 100%|██████████| 645/645 [00:00<00:00, 3859.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_tok_num: 181, lagger than max_test_seq_len: 128, test data will be split!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split_test_data = False\n",
    "# if max_tok_num > config[\"max_test_seq_len\"]:\n",
    "#     split_test_data = True\n",
    "#     print(\"max_tok_num: {}, lagger than max_test_seq_len: {}, test data will be split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "# else:\n",
    "#     print(\"max_tok_num: {}, less than or equal to max_test_seq_len: {}, no need to split!\".format(max_tok_num, config[\"max_test_seq_len\"]))\n",
    "# max_seq_len = min(max_tok_num, config[\"max_test_seq_len\"]) \n",
    "\n",
    "# if config[\"force_split\"]:\n",
    "#     split_test_data = True\n",
    "#     print(\"force to split the test dataset!\")    \n",
    "\n",
    "ori_test_data_dict = copy.deepcopy(test_data_dict)\n",
    "test_data_dict = {}\n",
    "max_seq_len_all_data = []\n",
    "for file_name, data in ori_test_data_dict.items():\n",
    "    split_data, max_seq_len_this_data = split(data, max_seq_len, sliding_len, file_name)\n",
    "    max_seq_len_all_data.append(max_seq_len_this_data)\n",
    "    test_data_dict[file_name] = split_data\n",
    "max_seq_len = max(max_seq_len_all_data)\n",
    "print(\"max_seq_len is reset to {}.\".format(max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data: 672\n"
     ]
    }
   ],
   "source": [
    "for filename, short_data in test_data_dict.items():\n",
    "    print(\"{}: {}\".format(filename, len(short_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder(Tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended visual_field is greater than current visual_field, reset to rec val: 8\n"
     ]
    }
   ],
   "source": [
    "meta = json.load(open(meta_path, \"r\", encoding = \"utf-8\"))\n",
    "tags = meta[\"tags\"]\n",
    "if meta[\"visual_field_rec\"] > visual_field:\n",
    "    visual_field = meta[\"visual_field_rec\"]\n",
    "    print(\"Recommended visual_field is greater than current visual_field, reset to rec val: {}\".format(visual_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "handshaking_tagger = HandshakingTaggingScheme(tags, max_seq_len, handshaking_kernel_config[\"visual_field\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = json.load(open(char2idx_path, \"r\", encoding = \"utf-8\"))\n",
    "def text2char_indices(text, max_seq_len = -1):\n",
    "    char_ids = []\n",
    "    chars = list(text)\n",
    "    for c in chars:\n",
    "        if c not in char2idx:\n",
    "            char_ids.append(char2idx['<UNK>'])\n",
    "        else:\n",
    "            char_ids.append(char2idx[c])\n",
    "    if len(char_ids) < max_seq_len:\n",
    "        char_ids.extend([char2idx['<PAD>']] * (max_seq_len - len(char_ids)))\n",
    "    if max_seq_len != -1:\n",
    "        char_ids = torch.tensor(char_ids[:max_seq_len]).long()\n",
    "    return char_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_bert:\n",
    "    data_maker = DataMaker(handshaking_tagger, word_tokenizer, text2char_indices, bert_tokenizer)\n",
    "else:\n",
    "    data_maker = DataMaker(handshaking_tagger, word_tokenizer, text2char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max word num, max subword num, max char num\n",
    "def cal_max_tok_num(data, tokenizer):\n",
    "    max_tok_num = 0\n",
    "    for example in data:\n",
    "        text = example[\"text\"]\n",
    "        max_tok_num = max(max_tok_num, len(tokenizer.tokenize(text)))\n",
    "    return max_tok_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max character num of a single word\n",
    "def get_max_char_num_in_subword(data):\n",
    "    max_char_num = 0\n",
    "    for example in data:\n",
    "        text = example[\"text\"]\n",
    "        subword2char_span = bert_tokenizer.encode_plus(text, \n",
    "                                                       return_offsets_mapping = True, \n",
    "                                                       add_special_tokens = False)[\"offset_mapping\"]\n",
    "        max_char_num = max([span[1] - span[0] for span in subword2char_span] + [max_char_num, ])\n",
    "    return max_char_num\n",
    "\n",
    "def get_max_char_num_in_word(data):\n",
    "    max_char_num = 0\n",
    "    for example in data:\n",
    "        text = example[\"text\"]\n",
    "        word2char_span = word_tokenizer.encode_plus(text)[\"offset_mapping\"]\n",
    "        max_char_num = max([span[1] - span[0] for span in word2char_span] + [max_char_num, ])\n",
    "    return max_char_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for data in list(test_data_dict.values()):\n",
    "    all_data.extend(data)\n",
    "\n",
    "# \n",
    "max_word_num = cal_max_tok_num(all_data, word_tokenizer)\n",
    "print(\"max_word_num: {}\".format(max_word_num))\n",
    "if use_bert:\n",
    "    max_subword_num = cal_max_tok_num(all_data, bert_tokenizer)\n",
    "    print(\"max_subword_num: {}\".format(max_subword_num))\n",
    "\n",
    "# max_char_num_in_tok   \n",
    "if use_bert:\n",
    "    max_char_num_in_tok = get_max_char_num_in_subword(all_data)\n",
    "else:\n",
    "    max_char_num_in_tok = get_max_char_num_in_word(all_data)\n",
    "print(\"max_char_num_in_tok: {}\".format(max_char_num_in_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if char_encoder_config is not None:\n",
    "    char_encoder_config[\"char_size\"] = len(char2idx)\n",
    "    char_encoder_config[\"max_char_num_in_tok\"] = max_char_num_in_tok\n",
    "if word_encoder_config is not None:\n",
    "    word_encoder_config[\"word2idx\"] = word2idx\n",
    "ent_extractor = TPLinkerNER(char_encoder_config,\n",
    "                            word_encoder_config,\n",
    "                            flair_config,\n",
    "                            handshaking_kernel_config,\n",
    "                            enc_hidden_size,\n",
    "                            activate_enc_fc,\n",
    "                            len(tags),\n",
    "                            bert_config,\n",
    "                            )\n",
    "ent_extractor = ent_extractor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics(handshaking_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model state paths\n",
    "model_state_dir = config[\"model_state_dict_dir\"]\n",
    "target_run_ids = set(config[\"run_ids\"])\n",
    "run_id2model_state_paths = {}\n",
    "for root, dirs, files in os.walk(model_state_dir):\n",
    "    for file_name in files:\n",
    "        run_id = root.split(\"-\")[-1]\n",
    "        if re.match(\".*model_state.*\\.pt\", file_name) and run_id in target_run_ids:\n",
    "            if run_id not in run_id2model_state_paths:\n",
    "                run_id2model_state_paths[run_id] = []\n",
    "            model_state_path = os.path.join(root, file_name)\n",
    "            run_id2model_state_paths[run_id].append(model_state_path)\n",
    "print(\"Following model states will be loaded: \")\n",
    "pprint(run_id2model_state_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_k_paths(path_list, k):\n",
    "    path_list = sorted(path_list, key = lambda x: int(re.search(\"(\\d+)\", x.split(\"/\")[-1]).group(1)))\n",
    "#     pprint(path_list)\n",
    "    return path_list[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only last k models\n",
    "k = config[\"last_k_model\"]\n",
    "for run_id, path_list in run_id2model_state_paths.items():\n",
    "    run_id2model_state_paths[run_id] = get_last_k_paths(path_list, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(ent_list):\n",
    "    ent_memory_set = set()\n",
    "    filtered_ent_list = []\n",
    "    for ent in ent_list:\n",
    "        ent_memory = \"{}\\u2E80{}\\u2E80{}\".format(ent[\"tok_span\"][0], ent[\"tok_span\"][1], ent[\"type\"])\n",
    "        if ent_memory not in ent_memory_set:\n",
    "            filtered_ent_list.append(ent)\n",
    "            ent_memory_set.add(ent_memory)\n",
    "    return filtered_ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, ori_test_data):\n",
    "    '''\n",
    "    test_data: if split, it would be samples with subtext\n",
    "    ori_test_data: the original data has not been split, used to get original text here\n",
    "    '''\n",
    "#     indexed_test_data = data_maker.get_indexed_data(test_data, max_seq_len, data_type = \"test\") # fill up to max_seq_len\n",
    "    if use_bert:\n",
    "        indexed_test_data = data_maker.get_indexed_data(test_data,\n",
    "                                                                max_word_num,\n",
    "                                                                max_char_num_in_tok, \n",
    "                                                                max_subword_num_train, \n",
    "                                                                data_type = \"test\")\n",
    "    else:\n",
    "        indexed_test_data = data_maker.get_indexed_data(test_data,\n",
    "                                                        max_word_num,\n",
    "                                                        max_char_num_in_tok, \n",
    "                                                        data_type = \"test\")\n",
    "    test_dataloader = DataLoader(MyDataset(indexed_test_data), \n",
    "                              batch_size = batch_size, \n",
    "                              shuffle = False, \n",
    "                              num_workers = 6,\n",
    "                              drop_last = False,\n",
    "                              collate_fn = lambda data_batch: data_maker.generate_batch(data_batch, use_bert = use_bert, data_type = \"test\"),\n",
    "                             )\n",
    "    \n",
    "    pred_sample_list = []\n",
    "    for batch_test_data in tqdm(test_dataloader, desc = \"Predicting\"):\n",
    "#         if config[\"encoder\"] == \"BERT\":\n",
    "#             sample_list, batch_input_ids, \\\n",
    "#             batch_attention_mask, batch_token_type_ids, \\\n",
    "#             tok2char_span_list, _ = batch_test_data\n",
    "\n",
    "#             batch_input_ids, \\\n",
    "#             batch_attention_mask, \\\n",
    "#             batch_token_type_ids = (batch_input_ids.to(device), \n",
    "#                                       batch_attention_mask.to(device), \n",
    "#                                       batch_token_type_ids.to(device))\n",
    "\n",
    "# #         elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "# #             text_id_list, text_list, batch_input_ids, tok2char_span_list = batch_test_data\n",
    "# #             batch_input_ids = batch_input_ids.to(device)\n",
    "     \n",
    "#         with torch.no_grad():\n",
    "#             if config[\"encoder\"] == \"BERT\":\n",
    "#                 batch_pred_shaking_outputs = ent_extractor(batch_input_ids, \n",
    "#                                                           batch_attention_mask, \n",
    "#                                                           batch_token_type_ids, \n",
    "#                                                          )\n",
    "# #             elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "# #                 batch_pred_shaking_outputs = ent_extractor(batch_input_ids)\n",
    "        if bert_config is not None:\n",
    "            sample_list, \\\n",
    "            padded_sent_list, \\\n",
    "            batch_subword_input_ids, \\\n",
    "            batch_attention_mask, \\\n",
    "            batch_token_type_ids, \\\n",
    "            tok2char_span_list, \\\n",
    "            batch_char_input_ids4subword, \\\n",
    "            batch_word_input_ids, \\\n",
    "            batch_subword2word_idx_map, \\\n",
    "            batch_gold_shaking_tag = batch_test_data\n",
    "        else:\n",
    "            sample_list, \\\n",
    "            padded_sent_list, \\\n",
    "            batch_char_input_ids4subword, \\\n",
    "            batch_word_input_ids, \\\n",
    "            tok2char_span_list, \\\n",
    "            batch_gold_shaking_tag = batch_test_data\n",
    "\n",
    "        batch_char_input_ids4subword, \\\n",
    "        batch_word_input_ids, \\\n",
    "        batch_gold_shaking_tag = (batch_char_input_ids4subword.to(device), \n",
    "                                  batch_word_input_ids.to(device),\n",
    "                                  batch_gold_shaking_tag.to(device) \n",
    "                                     )\n",
    "        if bert_config is not None:\n",
    "                batch_subword_input_ids, \\\n",
    "                batch_attention_mask, \\\n",
    "                batch_token_type_ids, \\\n",
    "                batch_subword2word_idx_map = (batch_subword_input_ids.to(device), \n",
    "                                              batch_attention_mask.to(device), \n",
    "                                              batch_token_type_ids.to(device), \n",
    "                                              batch_subword2word_idx_map.to(device))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if bert_config is not None:\n",
    "                batch_pred_shaking_outputs = ent_extractor(batch_char_input_ids4subword, \n",
    "                                                           batch_word_input_ids,\n",
    "                                                           padded_sent_list,\n",
    "                                                           batch_subword_input_ids, \n",
    "                                                           batch_attention_mask, \n",
    "                                                           batch_token_type_ids, \n",
    "                                                           batch_subword2word_idx_map)\n",
    "            else:\n",
    "                batch_pred_shaking_outputs = ent_extractor(batch_char_input_ids4subword, \n",
    "                                                           batch_word_input_ids,\n",
    "                                                           padded_sent_list\n",
    "                                                           )\n",
    "        batch_pred_shaking_tag = (batch_pred_shaking_outputs > 0.).long()\n",
    "\n",
    "        for ind in range(len(sample_list)):\n",
    "            sample = sample_list[ind]\n",
    "            text = sample[\"text\"]\n",
    "            text_id = sample[\"id\"]\n",
    "            tok2char_span = tok2char_span_list[ind]\n",
    "            pred_shaking_tag = batch_pred_shaking_tag[ind]\n",
    "            tok_offset, char_offset = 0, 0\n",
    "            if split_test_data:\n",
    "                tok_offset, char_offset = sample[\"tok_offset\"], sample[\"char_offset\"]\n",
    "            ent_list = handshaking_tagger.decode_ent(text, \n",
    "                                                     pred_shaking_tag, \n",
    "                                                     tok2char_span, \n",
    "                                                     tok_offset = tok_offset, \n",
    "                                                     char_offset = char_offset)\n",
    "            pred_sample_list.append({\n",
    "                \"text\": text,\n",
    "                \"id\": text_id,\n",
    "                \"entity_list\": ent_list,\n",
    "            })\n",
    "            \n",
    "    # merge\n",
    "    text_id2ent_list = {}\n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        if text_id not in text_id2ent_list:\n",
    "            text_id2ent_list[text_id] = sample[\"entity_list\"]\n",
    "        else:\n",
    "            text_id2ent_list[text_id].extend(sample[\"entity_list\"])\n",
    "\n",
    "    text_id2text = {sample[\"id\"]:sample[\"text\"] for sample in ori_test_data}\n",
    "    merged_pred_sample_list = []\n",
    "    for text_id, ent_list in text_id2ent_list.items():\n",
    "        merged_pred_sample_list.append({\n",
    "            \"id\": text_id,\n",
    "            \"text\": text_id2text[text_id],\n",
    "            \"entity_list\": filter_duplicates(ent_list),\n",
    "        })\n",
    "        \n",
    "    return merged_pred_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_prf(pred_sample_list, gold_test_data, pattern = \"only_head\"):\n",
    "    text_id2gold_n_pred = {}\n",
    "    for sample in gold_test_data:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id] = {\n",
    "            \"gold_entity_list\": sample[\"entity_list\"],\n",
    "        }\n",
    "    \n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id][\"pred_entity_list\"] = sample[\"entity_list\"]\n",
    "\n",
    "    correct_num, pred_num, gold_num = 0, 0, 0\n",
    "    for gold_n_pred in text_id2gold_n_pred.values():\n",
    "        gold_ent_list = gold_n_pred[\"gold_entity_list\"]\n",
    "        pred_ent_list = gold_n_pred[\"pred_entity_list\"] if \"pred_entity_list\" in gold_n_pred else []\n",
    "        if pattern == \"only_head_index\":\n",
    "            gold_ent_set = set([\"{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"type\"]) for ent in gold_ent_list])\n",
    "            pred_ent_set = set([\"{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"type\"]) for ent in pred_ent_list])\n",
    "        elif pattern == \"whole_span\":\n",
    "            gold_ent_set = set([\"{}\\u2E80{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"char_span\"][1], ent[\"type\"]) for ent in gold_ent_list])\n",
    "            pred_ent_set = set([\"{}\\u2E80{}\\u2E80{}\".format(ent[\"char_span\"][0], ent[\"char_span\"][1], ent[\"type\"]) for ent in pred_ent_list])\n",
    "        elif pattern == \"whole_text\":\n",
    "            gold_ent_set = set([\"{}\\u2E80{}\".format(ent[\"text\"], ent[\"type\"]) for ent in gold_ent_list])\n",
    "            pred_ent_set = set([\"{}\\u2E80{}\".format(ent[\"text\"], ent[\"type\"]) for ent in pred_ent_list])\n",
    "            \n",
    "        for ent_str in pred_ent_set:\n",
    "            if ent_str in gold_ent_set:\n",
    "                correct_num += 1\n",
    "\n",
    "        pred_num += len(pred_ent_set)\n",
    "        gold_num += len(gold_ent_set)\n",
    "#     print((correct_num, pred_num, gold_num))\n",
    "    prf = metrics.get_scores(correct_num, pred_num, gold_num)\n",
    "    return prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 4071.38it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: f824tncz, model state model_state_dict_33.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.73it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3676.09it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: f824tncz, model state model_state_dict_34.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.92it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 4087.67it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: f824tncz, model state model_state_dict_35.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.85it/s]\n",
      "Generate indexed data:   3%|▎         | 20/672 [00:00<00:04, 131.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1tlijgjd, model state model_state_dict_36.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 1652.83it/s]\n",
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.80it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 4161.30it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1tlijgjd, model state model_state_dict_37.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.79it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3560.04it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1tlijgjd, model state model_state_dict_38.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.62it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 4096.57it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1q0z9vz4, model state model_state_dict_31.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.78it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3655.89it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1q0z9vz4, model state model_state_dict_32.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.96it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 4157.91it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1q0z9vz4, model state model_state_dict_33.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.86it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3678.09it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 3hlyn8ty, model state model_state_dict_36.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.85it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3407.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 3hlyn8ty, model state model_state_dict_37.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.71it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3683.93it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 3hlyn8ty, model state model_state_dict_38.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.85it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 4075.63it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1z5th0xo, model state model_state_dict_24.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.67it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3668.07it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1z5th0xo, model state model_state_dict_25.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:06<00:00,  3.42it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3984.71it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1z5th0xo, model state model_state_dict_26.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.97it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 4032.03it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: sc6qtrn4, model state model_state_dict_33.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.81it/s]\n",
      "Generate indexed data:  41%|████      | 276/672 [00:00<00:00, 2755.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: sc6qtrn4, model state model_state_dict_34.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3119.81it/s]\n",
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.61it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3645.15it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: sc6qtrn4, model state model_state_dict_35.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.76it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3441.38it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2t7u61lp, model state model_state_dict_30.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.81it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3391.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2t7u61lp, model state model_state_dict_31.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.88it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 4021.15it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 2t7u61lp, model state model_state_dict_32.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.79it/s]\n",
      "Generate indexed data:  51%|█████     | 342/672 [00:00<00:00, 3416.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1yuufpge, model state model_state_dict_23.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 1553.80it/s]\n",
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.84it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3595.78it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1yuufpge, model state model_state_dict_24.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.85it/s]\n",
      "Generate indexed data: 100%|██████████| 672/672 [00:00<00:00, 3527.62it/s]\n",
      "Predicting:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1yuufpge, model state model_state_dict_25.pt loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 21/21 [00:05<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'../results/xf_agriculture/1q0z9vz4/test_data_res_31.json': 641,\n",
      " '../results/xf_agriculture/1q0z9vz4/test_data_res_32.json': 637,\n",
      " '../results/xf_agriculture/1q0z9vz4/test_data_res_33.json': 638,\n",
      " '../results/xf_agriculture/1tlijgjd/test_data_res_36.json': 642,\n",
      " '../results/xf_agriculture/1tlijgjd/test_data_res_37.json': 641,\n",
      " '../results/xf_agriculture/1tlijgjd/test_data_res_38.json': 641,\n",
      " '../results/xf_agriculture/1yuufpge/test_data_res_23.json': 642,\n",
      " '../results/xf_agriculture/1yuufpge/test_data_res_24.json': 641,\n",
      " '../results/xf_agriculture/1yuufpge/test_data_res_25.json': 642,\n",
      " '../results/xf_agriculture/1z5th0xo/test_data_res_24.json': 637,\n",
      " '../results/xf_agriculture/1z5th0xo/test_data_res_25.json': 641,\n",
      " '../results/xf_agriculture/1z5th0xo/test_data_res_26.json': 641,\n",
      " '../results/xf_agriculture/2t7u61lp/test_data_res_30.json': 641,\n",
      " '../results/xf_agriculture/2t7u61lp/test_data_res_31.json': 639,\n",
      " '../results/xf_agriculture/2t7u61lp/test_data_res_32.json': 641,\n",
      " '../results/xf_agriculture/3hlyn8ty/test_data_res_36.json': 642,\n",
      " '../results/xf_agriculture/3hlyn8ty/test_data_res_37.json': 643,\n",
      " '../results/xf_agriculture/3hlyn8ty/test_data_res_38.json': 643,\n",
      " '../results/xf_agriculture/f824tncz/test_data_res_33.json': 641,\n",
      " '../results/xf_agriculture/f824tncz/test_data_res_34.json': 641,\n",
      " '../results/xf_agriculture/f824tncz/test_data_res_35.json': 640,\n",
      " '../results/xf_agriculture/sc6qtrn4/test_data_res_33.json': 642,\n",
      " '../results/xf_agriculture/sc6qtrn4/test_data_res_34.json': 642,\n",
      " '../results/xf_agriculture/sc6qtrn4/test_data_res_35.json': 641}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "res_dict = {}\n",
    "predict_statistics = {}\n",
    "for file_name, short_data in test_data_dict.items():\n",
    "    ori_test_data = ori_test_data_dict[file_name]\n",
    "    for run_id, model_path_list in run_id2model_state_paths.items():\n",
    "        save_dir4run = os.path.join(save_res_dir, run_id)\n",
    "        if config[\"save_res\"] and not os.path.exists(save_dir4run):\n",
    "            os.makedirs(save_dir4run)\n",
    "            \n",
    "        for model_state_path in model_path_list:\n",
    "            res_num = re.search(\"(\\d+)\", model_state_path.split(\"/\")[-1]).group(1)\n",
    "            save_path = os.path.join(save_dir4run, \"{}_res_{}.json\".format(file_name, res_num))\n",
    "            \n",
    "            if os.path.exists(save_path):\n",
    "                pred_sample_list = [json.loads(line) for line in open(save_path, \"r\", encoding = \"utf-8\")]\n",
    "                print(\"{} already exists, load it directly!\".format(save_path))\n",
    "            else:\n",
    "                # load model state\n",
    "                model_state_dict = torch.load(model_state_path)\n",
    "                # if used paralell train, need to rm prefix \"module.\"\n",
    "                new_model_state_dict = OrderedDict()\n",
    "                for key, v in model_state_dict.items():\n",
    "                    key = re.sub(\"module\\.\", \"\", key)\n",
    "                    new_model_state_dict[key] = v\n",
    "                ent_extractor.load_state_dict(new_model_state_dict)\n",
    "                ent_extractor.eval()\n",
    "                print(\"run_id: {}, model state {} loaded\".format(run_id, model_state_path.split(\"/\")[-1]))\n",
    "\n",
    "                # predict\n",
    "                pred_sample_list = predict(short_data, ori_test_data)\n",
    "            \n",
    "            res_dict[save_path] = pred_sample_list\n",
    "            predict_statistics[save_path] = len([s for s in pred_sample_list if len(s[\"entity_list\"]) > 0])\n",
    "pprint(predict_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "if config[\"score\"]:\n",
    "    filepath2scores = {}\n",
    "    for file_path, pred_samples in res_dict.items():\n",
    "        file_name = re.match(\"(.*?)_res_\\d+.json\", file_path.split(\"/\")[-1]).group(1)\n",
    "        gold_test_data = ori_test_data_dict[file_name]\n",
    "        prf = get_test_prf(pred_samples, gold_test_data, pattern = config[\"correct\"])\n",
    "        filepath2scores[file_path] = prf\n",
    "    print(\"---------------- Results -----------------------\")\n",
    "    pprint(filepath2scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 159118.11it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 144592.52it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 160410.68it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 157049.00it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 165079.70it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 170704.57it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 167253.54it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 166913.01it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 174932.17it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 181944.05it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 182779.95it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 190368.45it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 184462.44it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 184638.69it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 190609.88it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 125096.00it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 192399.27it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 180403.18it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 196251.44it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 202146.46it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 198265.01it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 206104.38it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 202813.26it/s]\n",
      "check character level span: 100%|██████████| 645/645 [00:00<00:00, 178181.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# check char span\n",
    "for path, res in res_dict.items():\n",
    "    for sample in tqdm(res, \"check character level span\"):\n",
    "        text = sample[\"text\"]\n",
    "        for ent in sample[\"entity_list\"]:\n",
    "            assert ent[\"text\"] == text[ent[\"char_span\"][0]:ent[\"char_span\"][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Output: 100%|██████████| 645/645 [00:00<00:00, 39030.01it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 39862.76it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40571.17it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40608.93it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40214.74it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40288.40it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 39783.62it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40262.02it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 39929.83it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40172.34it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 39812.90it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40519.52it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40682.82it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40440.77it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40511.63it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40251.84it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40178.31it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40027.91it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 39874.51it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40272.21it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40567.52it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40246.45it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40169.36it/s]\n",
      "Output: 100%|██████████| 645/645 [00:00<00:00, 40821.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# save \n",
    "if config[\"save_res\"]:\n",
    "    for path, res in res_dict.items():\n",
    "        with open(path, \"w\", encoding = \"utf-8\") as file_out:\n",
    "            for sample in tqdm(res, desc = \"Output\"):\n",
    "                if len(sample[\"entity_list\"]) == 0:\n",
    "                    continue\n",
    "                json_line = json.dumps(sample, ensure_ascii = False)     \n",
    "                file_out.write(\"{}\\n\".format(json_line))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
