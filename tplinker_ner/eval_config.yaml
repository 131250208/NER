exp_name: genia
model_state_dict_dir: ./wandb
run_ids:
 - 1fa8u3qb

last_k_model: 1

data_home: ../data/biobert-large-cased-pubmed-58k
# biobert-large-cased-pubmed-58k
# bert-large-cased
# bert-base-cased
# biobert_v1.0_pubmed_pmc

test_data: "*test*.json"
meta: meta.json
word2idx: word2idx.json
char2idx: char2idx.json

device_num: 0

# >>>>>>>>Model>>>>>>>>>>>
## char encoder
use_char_encoder: true
max_char_num_in_tok: 16
char_encoder_config:
 emb_dim: 64
 emb_dropout: 0.1
 bilstm_layers: 
  - 1
  - 1
 bilstm_hidden_size: 
  - 64
  - 128
 bilstm_dropout:
  - 0.
  - 0.05
  - 0.

## bert: subword encoder
use_bert: true
bert_config:
 path: ../../pretrained_models/biobert-large-cased-pubmed-58k # biobert_v1.0_pubmed_pmc
 use_last_k_layers: 1
 finetune: true
## word encoder
use_word_encoder: true
word_encoder_config:
 emb_key: pubmed # pubmed, glove
 emb_dropout: 0.1
 bilstm_layers: 
  - 1
  - 1
 bilstm_hidden_size: 
  - 300
  - 600
 bilstm_dropout:
  - 0.
  - 0.05
  - 0.
 freeze_word_emb: false
## flair
use_flair: false
flair_embedding_ids:
 - pubmed-forward
 - pubmed-backward
## handshaking kernel
handshaking_kernel_config:
 visual_field: -1 # -1: use recommended value
 shaking_type: cln_plus
 context_type: lstm
## encoding fc
enc_hidden_size: 1024
activate_enc_fc: false

# predict config
batch_size: 32
max_seq_len: 256
sliding_len: 20
correct: whole_span # head_n_tail, only_head, whole_text

# results
save_res: false
save_res_dir: ../results
# score: set true only when test set tagged
score: true
